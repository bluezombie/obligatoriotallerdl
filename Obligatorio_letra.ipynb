{"cells":[{"cell_type":"markdown","metadata":{"id":"aZzGt0PbUIGG"},"source":["# Obligatorio - Taller de Deep Learning\n","\n","**Fecha de entrega:** 10/12/2024  \n","**Puntaje máximo:** 50 puntos  \n","\n","## Obligatorio\n","\n","El objetivo de este obligatorio es evaluar su conocimiento en Deep Learning mediante la implementación completa de un modelo de segmentación de imágenes basado en el paper [**\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"**](https://arxiv.org/pdf/1505.04597). Toda la implementación debe realizarse desde cero utilizando PyTorch, y los estudiantes tendrán la libertad de ajustar ciertos hiperparámetros y configuraciones mientras mantengan la esencia del paper original.\n","\n","### **Competencia en Kaggle**\n","\n","Además, como parte de este obligatorio, participarán en una competencia privada en Kaggle donde se les proporcionará un dataset de test oculto (sin target). Deberán subir sus predicciones a Kaggle y se evaluarán en función de la métrica **Dice Coefficient (Coeficiente de Dice)**. Esta competencia les permitirá comparar sus resultados con los de sus compañeros en un entorno real de evaluación.\n","\n","### **Qué es el Dice Coefficient?**\n","El **Dice Coefficient**, también conocido como F1-score para segmentación, es una métrica utilizada para evaluar la similitud entre la predicción y la verdad del terreno en tareas de segmentación. Se define de la siguiente manera:\n","\n","$$\n","\\text{Dice} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n","$$\n","\n","Donde:\n","- \\(A\\) es el conjunto de píxeles predichos como pertenecientes a la clase positiva.\n","- \\(B\\) es el conjunto de píxeles verdaderos pertenecientes a la clase positiva.\n","- \\(|A \\cap B|\\) es la intersección de \\(A\\) y \\(B\\), es decir, los píxeles correctamente predichos como positivos.\n","\n","Un valor de Dice de **1** indica una predicción perfecta, mientras que un valor de **0** indica que no hay coincidencia entre la predicción y el valor verdadero. Durante la competencia de Kaggle, deberán obtener un puntaje de al menos **0.7** en la métrica Dice para considerarse aprobados.\n","\n","### **Criterios a Evaluar**\n","\n","1. **Implementación Correcta del Modelo U-Net (20 puntos):**\n","   - Construcción de la arquitectura U-Net siguiendo la estructura descrita en el paper, permitiendo ajustes como el número de filtros, funciones de activación y métodos de inicialización de pesos.\n","   - Se aceptan mejoras como el uso de técnicas adicionales como batch normalization, otras funciones de activación, etc.\n","\n","2. **Entrenamiento del Modelo (10 puntos):**\n","   - Configuración adecuada del ciclo de entrenamiento, incluyendo la elección de la función de pérdida y del optimizador (Adam, SGD, etc.).\n","   - Uso de técnicas de regularización para mejorar la generalización del modelo, como el dropout, normalización de batch y data augmentation.\n","   - Gráficas y análisis de la evolución del entrenamiento, mostrando las curvas de pérdida y métricas relevantes tanto en el conjunto de entrenamiento como en el de validación.\n","\n","3. **Evaluación de Resultados (10 puntos):**\n","   - Evaluación exhaustiva del modelo utilizando métricas de segmentación como **Dice Coefficient**.\n","   - Análisis detallado de los resultados, incluyendo un análisis de errores para identificar y discutir casos difíciles.\n","   - Visualización de ejemplos representativos de segmentaciones correctas e incorrectas, comparando con las etiquetas manuales proporcionadas en el dataset.\n","\n","4. **Participación y Resultados en la Competencia Kaggle (5 puntos):**\n","   - Participación activa en la competencia de Kaggle, con al menos una (1) subida de predicción.\n","   - Puntaje obtenido en la tabla de posiciones de Kaggle, evaluado en base al **Dice Coefficient** en el conjunto de test oculto. Es necesario obtener al menos un valor de **0.7** para esta métrica.\n","\n","   Nota: El **Dice Coefficient** es la métrica utilizada para evaluar la precisión de los modelos de segmentación de imágenes en esta competencia. Un valor de Dice superior a 0.7 es requerido para aprobar esta tarea.\n","\n","### **Run-Length Encoding (RLE)**\n","\n","Dado que no se suben las imágenes segmentadas directamente a Kaggle, se requiere usar **Run-Length Encoding (RLE)** para comprimir las máscaras de predicción en una cadena de texto que será evaluada. El **RLE** es una técnica de compresión donde se representan secuencias consecutivas de píxeles en formato `start length`, indicando la posición de inicio y la longitud de cada secuencia de píxeles positivos.\n","\n","Para calcular el **RLE**, se sigue el siguiente proceso:\n","\n","1. Se aplanan las máscaras predichas en un solo vector\n","2. Se identifican los píxeles con valor positivo (1) y se calculan las secuencias consecutivas.\n","3. Se registra la posición de inicio de cada secuencia y su longitud en formato `start length`.\n","\n","Este formato comprimido se sube a Kaggle en lugar de las imágenes segmentadas.\n","\n","#### **Ejemplo de RLE**\n","\n","```python\n","import numpy as np\n","\n","def rle_encode(mask):\n","    pixels = np.array(mask).flatten(order='F')  # Aplanar la máscara en orden Fortran\n","    pixels = np.concatenate([[0], pixels, [0]])  # Añadir ceros al principio y final\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n","    runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n","    return ' '.join(str(x) for x in runs)\n","\n","mask = np.array([[0, 0, 1, 0, 0],\n","                 [0, 1, 1, 1, 0],\n","                 [1, 1, 1, 0, 0],\n","                 [0, 0, 0, 1, 1]])\n","\n","print(rle_encode(mask))\n","```\n","\n","> **Salida:** 3 1 6 2 9 3 14 1 16 1 20 1\n","\n","\n","### **Sobre el Dataset**\n","\n","El dataset proporcionado para esta tarea incluirá imágenes y máscaras para la segmentación de un conjunto específico de clases. El conjunto de entrenamiento estará disponible para su uso durante todo el proceso de desarrollo y pruebas, mientras que el conjunto de validación se mantendrá oculto para la evaluación final en Kaggle.\n","\n","### **Instrucciones de Entrega**\n","\n","- Deberán entregar un Jupyter Notebook (.ipynb) que contenga todo el código y las explicaciones necesarias para ejecutar la implementación, el entrenamiento y la evaluación del modelo.\n","- El notebook debe incluir secciones bien documentadas explicando las decisiones de diseño del modelo, los experimentos realizados, y los resultados obtenidos.\n","- El código debe estar escrito de manera clara.\n","- La entrega debe realizarse a través de la plataforma de gestión de ORT (gestion.ort.edu.uy) antes de la fecha límite.\n","\n","### **Materiales Adicionales**\n","\n","Para facilitar su trabajo, pueden consultar los siguientes recursos:\n","\n","- [U-Net: Convolutional Networks for Biomedical Image Segmentation (paper original)](https://arxiv.org/abs/1505.04597)\n","- [Documentación de PyTorch](https://pytorch.org/docs/stable/index.html)\n","- [Tutoriales y recursos adicionales en Kaggle](https://www.kaggle.com/)\n","\n","### **Competencia Kaggle**\n","\n","https://www.kaggle.com/t/9b4e546084034a59b182aac1ae892640"]},{"cell_type":"markdown","metadata":{"id":"8bDg55HyUIGR"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"EX1bBPyfUIGR"},"source":["\n","## Requisitos Universitarios\n","\n","Fecha de entrega: 10/12/2024 hasta las 21:00 horas en gestion.ort.edu.uy (max. 40Mb en formato zip)\n","\n","### Uso de material de apoyo y/o consulta\n","\n","Inteligencia Artificial Generativa:\n","\n","   - Seguir las pautas de los docentes: Se deben seguir las instrucciones específicas de los docentes sobre cómo utilizar la IA en cada curso.\n","   - Citar correctamente las fuentes y usos de IA: Siempre que se utilice una herramienta de IA para generar contenido, se debe citar adecuadamente la fuente y la forma en que se utilizó.\n","   - Verificar el contenido generado por la IA: No todo el contenido generado por la IA es correcto o preciso. Es esencial que los estudiantes verifiquen la información antes de usarla.\n","   - Ser responsables con el uso de la IA: Conocer los riesgos y desafíos, como la creación de “alucinaciones”, los peligros para la privacidad, las cuestiones de propiedad intelectual, los sesgos inherentes y la producción de contenido falso.\n","   - En caso de existir dudas sobre la autoría, plagio o uso no atribuido de IAG, el docente tendrá la opción de convocar al equipo de obligatorio a una defensa específica e individual sobre el tema.\n","\n","### Defensa\n","\n","Fecha de defensa: 11/12/2024\n","\n","La defensa es obligatoria y eliminatoria. El docente es quien definirá y comunicará la modalidad, y mecánica de defensa. La no presentación a la misma implica la pérdida de la totalidad de los puntos del Obligatorio.\n","\n","IMPORTANTE:\n","\n","   1) Inscribirse\n","   2) Formar grupos de hasta 2 personas del mismo dictado\n","   3) Subir el trabajo a Gestión antes de la hora indicada (ver hoja al final del documento: “RECORDATORIO”)\n","\n","Aquellos de ustedes que presenten alguna dificultad con su inscripción o tengan inconvenientes técnicos, por favor contactarse con el Coordinador de cursos o Coordinación adjunta antes de las 20:00h del día de la entrega, a través de los mails crosa@ort.edu.uy / posada_l@ort.edu.uy (matutino) / larrosa@ort.edu.uy (nocturno), o vía Ms Teams."]},{"cell_type":"markdown","metadata":{"id":"8bH1rWpba_Go"},"source":["# Preparación entorno"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXb6EO3JVPsM","outputId":"a55e1602-bf10-4e52-c08a-7e941ff90715","executionInfo":{"status":"ok","timestamp":1733762642336,"user_tz":180,"elapsed":259,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtegMesabVSN","outputId":"b7c8c3c9-5c14-4dac-a606-3246f9d2c44d","executionInfo":{"status":"ok","timestamp":1733762861761,"user_tz":180,"elapsed":12971,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["!pip install torch torchinfo\n","!pip install scikit-learn\n","!pip install kaggle\n","!pip install matplotlib\n","!pip install pandas"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"X80xVHuDmHhI","executionInfo":{"status":"ok","timestamp":1733762865634,"user_tz":180,"elapsed":569,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["# Definimos el entorno para interactuar con Kaggle\n","# Debemos subir el archivo kaggle.json previamente\n","!mkdir -p /root/.config/kaggle\n","!mv kaggle.json /root/.config/kaggle/\n","!chmod 600 /root/.config/kaggle/kaggle.json"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ljwM0SF0bBat","executionInfo":{"status":"ok","timestamp":1733763479211,"user_tz":180,"elapsed":229,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split, ConcatDataset, Subset\n","from torchvision.utils import make_grid\n","import torchvision.datasets as datasets\n","\n","from torchvision.transforms import v2 as T\n","from torchvision.io import read_image, ImageReadMode\n","\n","from torchinfo import summary\n","\n","import matplotlib.pyplot as plt\n","\n","import os\n","from pathlib import Path\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n",")\n","\n","import numpy as np\n","\n","import sys\n","import platform\n","\n","import PIL\n","from PIL import Image\n","\n","import kaggle\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","from tqdm.notebook import tqdm_notebook\n","\n","from utils import (\n","    train,\n","    train_unet,\n","    model_calassification_report,\n","    show_tensor_image,\n","    show_tensor_images,\n","    print_log,\n","    print_log_unet,\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZslx9wPbhBO","outputId":"14ff5ef3-8271-4e55-fb04-46695aaff735","executionInfo":{"status":"ok","timestamp":1733763485718,"user_tz":180,"elapsed":213,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","Num Workers: 0\n"]}],"source":["# definimos el dispositivo que vamos a usar\n","DEVICE = \"cpu\"  # por defecto, usamos la CPU\n","if torch.cuda.is_available():\n","    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n","elif torch.backends.mps.is_available():\n","    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n","\n","#NUM_WORKERS = max(os.cpu_count() - 1, 1)\n","NUM_WORKERS = 0\n","\n","# DEVICE = \"cpu\"\n","print(f\"Device: {DEVICE}\")\n","print(f\"Num Workers: {NUM_WORKERS}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NfQXb6nkXvfj","outputId":"5cfd5676-a232-4a02-9844-bf4858876a6d","executionInfo":{"status":"ok","timestamp":1733757154641,"user_tz":180,"elapsed":413,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec  9 15:12:34 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0              44W / 400W |      5MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["# De acuerdo a la documentación de Colab, obtenemos información de la GPU\n","# en caso de contar con ella.\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"h-_obzs1dt2W","executionInfo":{"status":"ok","timestamp":1733762895496,"user_tz":180,"elapsed":228,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["# Constantes\n","\n","SEED = 34\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","BATCH_SIZE = 32"]},{"cell_type":"markdown","metadata":{"id":"9uIx_1A4Y3cP"},"source":["# Dataset, dataloader y datos de entrenamiento"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"AnmE9vXdY5O2","executionInfo":{"status":"ok","timestamp":1733762897662,"user_tz":180,"elapsed":248,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["class PeopleDataset(Dataset):\n","    def __init__(self, data, masks=None, img_transforms=None, mask_transforms=None):\n","      self.train_data = data\n","      self.train_masks = masks\n","      self.img_transforms = img_transforms\n","      self.mask_transforms = mask_transforms\n","\n","      # Es necesario ordenar la lista para que las imágenes\n","      # sean coherentes con las máscaras.\n","      self.images = sorted(os.listdir(self.train_data))\n","      self.masks = sorted(os.listdir(self.train_masks))\n","\n","\n","    def __len__(self):\n","      if self.train_masks is not None:\n","        assert len(self.images) == len(self.masks), 'incompatibilidad de imágenes y máscaras'\n","      return len(self.images)\n","\n","    def __getitem__(self, idx):\n","      # Obtenemos los paths de las imágenes\n","      img_path = os.path.join(self.train_data, self.images[idx])\n","\n","      if self.train_masks is not None:\n","        mask_path = os.path.join(self.train_masks, self.masks[idx])\n","\n","      image = read_image(img_path)\n","\n","      if self.train_masks is not None:\n","        mask = read_image(mask_path)\n","\n","      # Verificamos si existen transformaciones, y en caso que sí\n","      # las aplicamos\n","      if self.img_transforms:\n","        image = self.img_transforms(image)\n","      if self.train_masks is not None:\n","        if self.mask_transforms is not None:\n","          mask = self.mask_transforms(mask)\n","\n","\n","      img = image.to(DEVICE)\n","      # Nos quedamos con un solo canal de la imagen, ya que todos son iguales\n","      if self.train_masks is not None:\n","        mask = mask[0:1, :, :]\n","        msk = mask.to(DEVICE)\n","\n","\n","      # if img.shape != (3, 224, 224):\n","      #   print(f\"Imagen con shape incorrecto: {self.images[idx]}\")\n","      # if self.train_masks is not None:\n","      #   if msk.shape != (1, 224, 224):\n","      #     print(msk.shape)\n","      #     print(\"Wrong mask shape!\")\n","\n","      return img, msk"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaHnkDKwjjES","outputId":"a1baffc9-901f-4710-f425-4c5b31851b7c","executionInfo":{"status":"ok","timestamp":1733763519765,"user_tz":180,"elapsed":18602,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading tdl-segmentacion.zip to /content\n"," 99% 2.12G/2.14G [00:16<00:00, 190MB/s]\n","100% 2.14G/2.14G [00:17<00:00, 134MB/s]\n"]}],"source":["# Descargamos el dataset\n","!kaggle competitions download -c tdl-segmentacion"]},{"cell_type":"code","source":["!rm -rf ./test\n","!rm -rf ./train\n","!rm -f tdl-segmentacion.zip"],"metadata":{"id":"R_LVQazyWh6B","executionInfo":{"status":"ok","timestamp":1733763440801,"user_tz":180,"elapsed":442,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"4UHevwfqn4KL","executionInfo":{"status":"ok","timestamp":1733763573835,"user_tz":180,"elapsed":18619,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60c16a2f-1195-4474-a488-3e22c2bd71d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat './tdl-segmentacion/test': No such file or directory\n","mv: cannot stat './tdl-segmentacion/train': No such file or directory\n","rmdir: failed to remove './tdl-segmentacion': No such file or directory\n"]}],"source":["# Descomprimimos el dataset\n","!unzip -q tdl-segmentacion.zip\n","!mv ./tdl-segmentacion/test .\n","!mv ./tdl-segmentacion/train .\n","!rmdir ./tdl-segmentacion"]},{"cell_type":"code","source":["# Eliminamos imágenes que no cumplen con el requerimiento\n","# de contar con 3 canales (RGB)\n","# Estas imágenes fueron detectadas durante las etapas\n","# de entrenamiento de pruebas.\n","!rm -f train/images/1775.png\n","!rm -f train/masks/1775.png\n","!rm -f train/images/1733.png\n","!rm -f train/masks/1733.png"],"metadata":{"id":"mzaO5-RMRJdH","executionInfo":{"status":"ok","timestamp":1733763576812,"user_tz":180,"elapsed":450,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"id":"xxwANDv0ZPYm","executionInfo":{"status":"ok","timestamp":1733763581180,"user_tz":180,"elapsed":213,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["TRAIN_PATH = 'train/images/'\n","TRAIN_MASK_PATH = 'train/masks/'\n","TEST_PATH = 'test/images/'\n","\n","transform_image_data = T.Compose([\n","   T.Resize((224, 224))])\n","\n","transform_mask_data = T.Compose([\n","    T.Resize((224, 224))])\n","\n","full_dataset = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=transform_image_data, mask_transforms=transform_mask_data)\n","\n","TRAIN_SIZE = int(len(full_dataset)*0.8)\n","VAL_SIZE = len(full_dataset) - TRAIN_SIZE\n","\n","train_dataset, val_dataset = random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE])\n","\n","test_dataset = PeopleDataset(data=TEST_PATH, masks=None, img_transforms=transform_image_data, mask_transforms=None)\n","\n","\n","def get_data_loaders(batch_size, train_dataset, val_dataset, test_dataset):\n","\n","    train_loader = DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n","    )\n","\n","    test_loader = DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n","    )\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"c9ZHzYRCp3ir","executionInfo":{"status":"ok","timestamp":1733763584899,"user_tz":180,"elapsed":219,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["# Obtenemos los dataloaders\n","train_loader, val_loader, test_loader = get_data_loaders(BATCH_SIZE, train_dataset, val_dataset, test_dataset)"]},{"cell_type":"markdown","source":["Vamos a explorar los datos, y asegurarnos de que todas las imágenes, posean\n","la cantidad correcta de canales."],"metadata":{"id":"9LjBR8Zijq9p"}},{"cell_type":"code","source":["def explore_datasets(dataloader):\n","  # Vamos a explorar los canales del dataset para asegurarnos que todos\n","  # tengan las dimensiones correctas\n","  for batch_idx, (data, target) in enumerate(train_loader):\n","    # print(f\"Batch {batch_idx + 1}\")\n","    for i in range(len(data)):\n","      # print(f\"Channel {i + 1}\")\n","      if data[i].shape != (3, 224, 224):\n","        print(\"Wrong shape!\")\n","        print(data[i])"],"metadata":{"id":"8qJSOxkHjj4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explore_datasets(train_loader)"],"metadata":{"id":"YYjEsQUdjxdp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como puede observarse, existen imágenes que no poseen los canales correctos. Se\n","procede a eliminar dichas imágenes, y a volver a generar el dataset."],"metadata":{"id":"ItNaWhpzj3ZI"}},{"cell_type":"code","source":["!rm -f train/images/1775.png\n","!rm -f train/masks/1775.png\n","!rm -f train/images/1733.png\n","!rm -f train/masks/1733.png"],"metadata":{"id":"GcA5jeVVj_hC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Volvemos a generar los datasets correctamente, sin las imágenes incorrectoas."],"metadata":{"id":"amg5SYe_kIPE"}},{"cell_type":"code","source":["full_dataset = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=transform_image_data, mask_transforms=transform_mask_data)\n","train_dataset, val_dataset = random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE])\n","test_dataset = PeopleDataset(data=TEST_PATH, masks=None, img_transforms=transform_image_data, mask_transforms=None)\n","train_loader, val_loader, test_loader = get_data_loaders(BATCH_SIZE, train_dataset, val_dataset, test_dataset)"],"metadata":{"id":"pZJ593AvkH6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNOFFuxWXi_V","outputId":"bdac149e-d907-4304-a926-903bab37182c","executionInfo":{"status":"ok","timestamp":1733763587793,"user_tz":180,"elapsed":1270,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([24748, 25428], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([39419, 10757], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([33873, 16303], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([45078,  5098], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([23403, 26773], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([44750,  5426], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([41220,  8956], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([15844, 34332], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([23973, 26203], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([32139, 18037], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([25740, 24436], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([45159,  5017], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([21528, 28648], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([21643, 28533], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([21912, 28264], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([30613, 19563], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([24705, 25471], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([42579,  7597], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([25092, 25084], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([46500,  3676], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([44153,  6023], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([39028, 11148], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([16764, 33412], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([34685, 15491], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([48561,  1615], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([44752,  5424], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([32622, 17554], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([16821, 33355], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([35659, 14517], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([18999, 31177], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([42680,  7496], device='cuda:0')\n","torch.Size([3, 224, 224]) torch.Size([1, 224, 224])\n","tensor([0, 1], device='cuda:0', dtype=torch.uint8)\n","tensor([47616,  2560], device='cuda:0')\n","torch.Size([32, 3, 224, 224]) torch.Size([32, 1, 224, 224])\n"]}],"source":["# Testeamos el dataloader obteniendo un minimatch y verificando las\n","# dimensiones y también los valores de las máscaras\n","imgs, masks = next(iter(train_loader))\n","for i in range(len(imgs)):\n","  print(imgs[i].shape, masks[i].shape)\n","  unique_values, counts = torch.unique(masks[i], return_counts=True)\n","  print(unique_values)  # Output: tensor([1, 2, 3, 4, 5, 6])\n","  print(counts)         # Output: tensor([4, 1, 1, 1, 1, 1])  (counts of each unique value)\n","\n","print(imgs.shape, masks.shape)"]},{"cell_type":"markdown","source":["Como puede observarse, las imágenes y las máscaras poseen las dimensiones correctas. En particular, las imágenes poseen 3x224x224 y las máscaras poseen 1x224x224.\n","\n","Vamos a continuación mostrar las imágenes, y sus respectivas máscaras del conjunto de entrenamiento. Validaremos que las máscaras son correctas, tomando una muestra de éstos datos."],"metadata":{"id":"5_iHT94Glju-"}},{"cell_type":"code","execution_count":30,"metadata":{"id":"ZkwiJr87n2-_","executionInfo":{"status":"ok","timestamp":1733763591569,"user_tz":180,"elapsed":211,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["def plot_mini_batch(imgs, masks, show_mask=True):\n","  '''\n","  Función para mostrar un mini batch de imágenes y máscaras\n","  '''\n","  plt.figure(figsize=(20,10))\n","  for i in range(BATCH_SIZE):\n","    plt.subplot(4, 8, i+1)\n","    # Permutamos ya que el tensor posee la siguiente información:\n","    #  - (canal, alto, ancho)\n","    # Y la función para mostrar la imgen espera:\n","    # - (alta, ancho, canal)\n","    # Por este motivo, se realiza la permutación\n","    img=imgs[i,...].permute(1,2,0).to(DEVICE).cpu().numpy()\n","    mask=masks[i,...].permute(1,2,0).to(DEVICE).cpu().numpy()\n","    plt.imshow(img)\n","    plt.axis('off')\n","    # Incluimos la máscara\n","    if show_mask:\n","      plt.imshow(mask, alpha=0.4, cmap='gray',vmin=0,vmax=1)\n","    plt.axis('off')\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dOZX6MpWniT7vEbSO5C8u-EoOnB-7TMX"},"id":"c0_7QKi3Vc6T","outputId":"b9bdee4d-87de-4a7a-a7c3-df7c31fff286","executionInfo":{"status":"ok","timestamp":1733763602908,"user_tz":180,"elapsed":3857,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Mostramos un mini batch de las imágenes\n","plot_mini_batch(imgs, masks, True)"]},{"cell_type":"markdown","source":["Como puede observarse, las imágenes muestran sus máscaras de forma correcta, indicando la precisión de éstas máscaras con respecto a las imágenes."],"metadata":{"id":"MAvSuDrnmBMb"}},{"cell_type":"markdown","metadata":{"id":"vDlCkTOEUe14"},"source":["# Modelo\n"]},{"cell_type":"markdown","metadata":{"id":"DcL2_-3nw6HE"},"source":["Dada la estructura de la U-Net, y la repetición de los bloques de\n","convolución tanto en la sección de down-sampling como en la de\n","up-sampling, crearemos clases con la estructura de la\n","convolución. De esta forma, simplificaremos el\n","código parametrizando dichas convoluciones\n","de acuerdo a la etapa de la red en la\n","que nos encontremos."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-gSjF2zCWv1F","executionInfo":{"status":"ok","timestamp":1733763008713,"user_tz":180,"elapsed":218,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[],"source":["class Conv_3_k(nn.Module):\n","  '''\n","  Bloque con una convluición de 3x3, que será la base para el desarrollo\n","  de todo el modelo.\n","  '''\n","  def __init__(self, channels_in, channels_out):\n","    super().__init__()\n","    # Utilizaremos padding para que las imágenes no se modifiquen\n","    self.conv1 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1)\n","  def forward(self, x):\n","    return self.conv1(x)\n","\n","class Double_Conv(nn.Module):\n","    '''\n","    Bloque con doble convolución, que será utilizado en el encoder y decoder.\n","    '''\n","    def __init__(self, channels_in, channels_out):\n","      super().__init__()\n","      self.double_conv = nn.Sequential(\n","                           Conv_3_k(channels_in, channels_out),\n","                           nn.BatchNorm2d(channels_out),\n","                           nn.ReLU(),\n","                           Conv_3_k(channels_out, channels_out),\n","                           nn.BatchNorm2d(channels_out),\n","                           nn.ReLU(),\n","                            )\n","    def forward(self, x):\n","      return self.double_conv(x)\n","\n","class Down_Conv(nn.Module):\n","    '''\n","    Rama de la U-Net responsable del encoder (down sampling).\n","    '''\n","    def __init__(self, channels_in, channels_out):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","                        nn.MaxPool2d(2,2),\n","                        Double_Conv(channels_in, channels_out)\n","                        )\n","    def forward(self, x):\n","        return self.encoder(x)\n","\n","class Up_Conv(nn.Module):\n","    '''\n","    Sección de la U-Net responsable del up sampling.\n","    La capa Upsample es responsable de escalar la imagen -en este caso-\n","    por un factor de 2.\n","    Luego, en el forward, concatenamos esta imagen \"upscaled\"\n","    con la salida del encoder que se guardó en una variable\n","    previamente.\n","    '''\n","    def __init__(self,channels_in, channels_out):\n","        super().__init__()\n","        self.upsample_layer = nn.Sequential(\n","                        nn.Upsample(scale_factor=2, mode='bicubic'),\n","                        nn.Conv2d(channels_in, channels_in//2, kernel_size=1, stride=1)\n","                        )\n","        self.decoder = Double_Conv(channels_in, channels_out)\n","\n","    def forward(self, x1, x2):\n","        '''\n","        x1 - salida escalada por un factor de 2\n","        x2 - salida que proviene del encoder\n","        '''\n","        x1 = self.upsample_layer(x1)\n","        x = torch.cat([x2, x1],dim=1)\n","        return self.decoder(x)\n","\n","class UNET(nn.Module):\n","    '''\n","    Definición de la U-Net utilizando los bloques anteriormente definidos\n","    '''\n","    def __init__(self, channels_in, channels, num_classes):\n","        super().__init__()\n","        self.first_conv = Double_Conv(channels_in, channels) #64, 224, 224\n","        self.down_conv1 = Down_Conv(channels, 2*channels) # 128, 112, 112\n","        self.down_conv2 = Down_Conv(2*channels, 4*channels) # 256, 56, 56\n","        self.down_conv3 = Down_Conv(4*channels, 8*channels) # 512, 28, 28\n","\n","        self.middle_conv = Down_Conv(8*channels, 16*channels) # 1024, 14, 14\n","\n","        self.up_conv1 = Up_Conv(16*channels, 8*channels)\n","        self.up_conv2 = Up_Conv(8*channels, 4*channels)\n","        self.up_conv3 = Up_Conv(4*channels, 2*channels)\n","        self.up_conv4 = Up_Conv(2*channels, channels)\n","\n","        self.last_conv = nn.Conv2d(channels, num_classes, kernel_size=1, stride=1)\n","\n","    def forward(self, x):\n","        x1 = self.first_conv(x)\n","        x2 = self.down_conv1(x1)\n","        x3 = self.down_conv2(x2)\n","        x4 = self.down_conv3(x3)\n","\n","        x5 = self.middle_conv(x4)\n","\n","        u1 = self.up_conv1(x5, x4)\n","        u2 = self.up_conv2(u1, x3)\n","        u3 = self.up_conv3(u2, x2)\n","        u4 = self.up_conv4(u3, x1)\n","\n","        return self.last_conv(u4)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPmCKtrbZXml","outputId":"ea51e33e-8b19-4d88-840b-05e902eb0ee8","executionInfo":{"status":"ok","timestamp":1733763013894,"user_tz":180,"elapsed":1643,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","UNET                                          [32, 2, 224, 224]         --\n","├─Double_Conv: 1-1                            [32, 64, 224, 224]        --\n","│    └─Sequential: 2-1                        [32, 64, 224, 224]        --\n","│    │    └─Conv_3_k: 3-1                     [32, 64, 224, 224]        1,792\n","│    │    └─BatchNorm2d: 3-2                  [32, 64, 224, 224]        128\n","│    │    └─ReLU: 3-3                         [32, 64, 224, 224]        --\n","│    │    └─Conv_3_k: 3-4                     [32, 64, 224, 224]        36,928\n","│    │    └─BatchNorm2d: 3-5                  [32, 64, 224, 224]        128\n","│    │    └─ReLU: 3-6                         [32, 64, 224, 224]        --\n","├─Down_Conv: 1-2                              [32, 128, 112, 112]       --\n","│    └─Sequential: 2-2                        [32, 128, 112, 112]       --\n","│    │    └─MaxPool2d: 3-7                    [32, 64, 112, 112]        --\n","│    │    └─Double_Conv: 3-8                  [32, 128, 112, 112]       221,952\n","├─Down_Conv: 1-3                              [32, 256, 56, 56]         --\n","│    └─Sequential: 2-3                        [32, 256, 56, 56]         --\n","│    │    └─MaxPool2d: 3-9                    [32, 128, 56, 56]         --\n","│    │    └─Double_Conv: 3-10                 [32, 256, 56, 56]         886,272\n","├─Down_Conv: 1-4                              [32, 512, 28, 28]         --\n","│    └─Sequential: 2-4                        [32, 512, 28, 28]         --\n","│    │    └─MaxPool2d: 3-11                   [32, 256, 28, 28]         --\n","│    │    └─Double_Conv: 3-12                 [32, 512, 28, 28]         3,542,016\n","├─Down_Conv: 1-5                              [32, 1024, 14, 14]        --\n","│    └─Sequential: 2-5                        [32, 1024, 14, 14]        --\n","│    │    └─MaxPool2d: 3-13                   [32, 512, 14, 14]         --\n","│    │    └─Double_Conv: 3-14                 [32, 1024, 14, 14]        14,161,920\n","├─Up_Conv: 1-6                                [32, 512, 28, 28]         --\n","│    └─Sequential: 2-6                        [32, 512, 28, 28]         --\n","│    │    └─Upsample: 3-15                    [32, 1024, 28, 28]        --\n","│    │    └─Conv2d: 3-16                      [32, 512, 28, 28]         524,800\n","│    └─Double_Conv: 2-7                       [32, 512, 28, 28]         --\n","│    │    └─Sequential: 3-17                  [32, 512, 28, 28]         7,080,960\n","├─Up_Conv: 1-7                                [32, 256, 56, 56]         --\n","│    └─Sequential: 2-8                        [32, 256, 56, 56]         --\n","│    │    └─Upsample: 3-18                    [32, 512, 56, 56]         --\n","│    │    └─Conv2d: 3-19                      [32, 256, 56, 56]         131,328\n","│    └─Double_Conv: 2-9                       [32, 256, 56, 56]         --\n","│    │    └─Sequential: 3-20                  [32, 256, 56, 56]         1,771,008\n","├─Up_Conv: 1-8                                [32, 128, 112, 112]       --\n","│    └─Sequential: 2-10                       [32, 128, 112, 112]       --\n","│    │    └─Upsample: 3-21                    [32, 256, 112, 112]       --\n","│    │    └─Conv2d: 3-22                      [32, 128, 112, 112]       32,896\n","│    └─Double_Conv: 2-11                      [32, 128, 112, 112]       --\n","│    │    └─Sequential: 3-23                  [32, 128, 112, 112]       443,136\n","├─Up_Conv: 1-9                                [32, 64, 224, 224]        --\n","│    └─Sequential: 2-12                       [32, 64, 224, 224]        --\n","│    │    └─Upsample: 3-24                    [32, 128, 224, 224]       --\n","│    │    └─Conv2d: 3-25                      [32, 64, 224, 224]        8,256\n","│    └─Double_Conv: 2-13                      [32, 64, 224, 224]        --\n","│    │    └─Sequential: 3-26                  [32, 64, 224, 224]        110,976\n","├─Conv2d: 1-10                                [32, 2, 224, 224]         130\n","===============================================================================================\n","Total params: 28,954,626\n","Trainable params: 28,954,626\n","Non-trainable params: 0\n","Total mult-adds (T): 1.18\n","===============================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 14103.87\n","Params size (MB): 115.82\n","Estimated Total Size (MB): 14238.96\n","==============================================================================================="]},"metadata":{},"execution_count":17}],"source":["# Mostramos a continuación un resumen del modelo definido, considerando\n","# imágenes de entrada de 224x224, 64 canales de procesamiento\n","# para la U-Net, y 3 canales para las imágenes considerando\n","# R, G y B.\n","summary(UNET(3, 64, 2), input_size=(BATCH_SIZE, 3, 224, 224))"]},{"cell_type":"markdown","metadata":{"id":"Pso29u7yZnZt"},"source":["Como puede observarse en el resumen, el modelo y su arquitectura hacen computacionalmente pesado el proceso. De lo anterior, puede observarse\n","que se requieren unos 14 GB de memoria RAM aproximadamente por cada\n","\"forward\" de un batch, contemplando imágenes de 224 x 224. Esto podría tener implicancias en el entrenamiento\n","ya que dados los recursos de hardware, es posible que se tengan que\n","variar tanto el BATCH_SIZE, como el tamaño de las imágenes a procesar."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"czP9VsleMPsA","outputId":"9672b767-2cfc-4833-ecc1-006e1c6b9926","executionInfo":{"status":"ok","timestamp":1733763053884,"user_tz":180,"elapsed":9510,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 2, 224, 224])\n"]}],"source":["# Definimos una función de test, y observamos el shape\n","# del modelo luego de la predicción\n","def test():\n","    x = torch.randn((32, 3, 224, 224))\n","    model = UNET(3, 64, 2)\n","    return model(x)\n","\n","preds = test()\n","print(preds.shape)"]},{"cell_type":"markdown","metadata":{"id":"0TqOT9izaWmV"},"source":["# Entrenamiento\n","\n","Probemos inicialmente un entrenamiento del modelo, sin realizar ninguna manipulación de las imágenes más allá de modificar su tamaño a 224x224 para mantener un tamaño reducido y no sobrecargar la memoria.\n","\n","Se utilizará como función de cálculo de error \"Cross Entropy\" y \"SGD\" para el cálculo del descenso del gradiente (dada la complejidad del modelo, es altamente recomendable utilizar ésta forma de cálculo frente a otras)."]},{"cell_type":"code","source":["def release_resources():\n","  # Definimos una función para liberar recursos cuando\n","  # nos encontramos realizando pruebas.\n","  # Assuming `model` and `optimizer` are your objects\n","  # del model2\n","  # del optimizer\n","  # Force garbage collection\n","  import gc\n","  gc.collect()\n","  # Clear CUDA memory\n","  torch.cuda.empty_cache()\n","\n","  # Optional: Print current GPU memory usage\n","  torch.cuda.memory_summary(device=DEVICE, abbreviated=False)\n","\n"],"metadata":{"id":"QEbHCb4PJN_K","executionInfo":{"status":"ok","timestamp":1733764037083,"user_tz":180,"elapsed":239,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["release_resources()"],"metadata":{"id":"oufzAejRJhGF","executionInfo":{"status":"ok","timestamp":1733764038629,"user_tz":180,"elapsed":245,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XZ2cZFVMajtW","outputId":"09f900a3-50e0-49c5-bc2d-0800edf589fc","executionInfo":{"status":"ok","timestamp":1733763264381,"user_tz":180,"elapsed":150910,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001 | Train Loss: 0.66240 | Val Loss: 0.68707 | Accuracy: 0.61804 | Dice: 0.49156\n","Epoch: 002 | Train Loss: 0.60299 | Val Loss: 0.68606 | Accuracy: 0.60048 | Dice: 0.54848\n","Epoch: 003 | Train Loss: 0.58616 | Val Loss: 0.68337 | Accuracy: 0.61818 | Dice: 0.59640\n","Epoch: 004 | Train Loss: 0.56860 | Val Loss: 0.66512 | Accuracy: 0.67636 | Dice: 0.67812\n","Epoch: 005 | Train Loss: 0.55794 | Val Loss: 0.65218 | Accuracy: 0.67442 | Dice: 0.69502\n","Epoch: 006 | Train Loss: 0.54123 | Val Loss: 0.64058 | Accuracy: 0.64629 | Dice: 0.51793\n","Epoch: 007 | Train Loss: 0.54090 | Val Loss: 0.60307 | Accuracy: 0.70965 | Dice: 0.66239\n","Epoch: 008 | Train Loss: 0.52693 | Val Loss: 0.59959 | Accuracy: 0.69945 | Dice: 0.63252\n","Epoch: 009 | Train Loss: 0.52616 | Val Loss: 0.61146 | Accuracy: 0.70169 | Dice: 0.63074\n","Epoch: 010 | Train Loss: 0.52689 | Val Loss: 0.58958 | Accuracy: 0.71001 | Dice: 0.65269\n","Epoch: 011 | Train Loss: 0.50126 | Val Loss: 0.59113 | Accuracy: 0.71454 | Dice: 0.69760\n","Epoch: 012 | Train Loss: 0.49480 | Val Loss: 0.70690 | Accuracy: 0.68176 | Dice: 0.64813\n","Epoch: 013 | Train Loss: 0.50883 | Val Loss: 0.60753 | Accuracy: 0.72651 | Dice: 0.70827\n","Epoch: 014 | Train Loss: 0.47113 | Val Loss: 0.57501 | Accuracy: 0.75468 | Dice: 0.69493\n","Epoch: 015 | Train Loss: 0.46071 | Val Loss: 0.65262 | Accuracy: 0.67454 | Dice: 0.69891\n","Epoch: 016 | Train Loss: 0.45870 | Val Loss: 0.69988 | Accuracy: 0.66450 | Dice: 0.70490\n","Epoch: 017 | Train Loss: 0.45997 | Val Loss: 0.55969 | Accuracy: 0.75343 | Dice: 0.75237\n","Epoch: 018 | Train Loss: 0.46812 | Val Loss: 0.67522 | Accuracy: 0.69740 | Dice: 0.63484\n","Epoch: 019 | Train Loss: 0.43715 | Val Loss: 0.50652 | Accuracy: 0.79172 | Dice: 0.79307\n","Epoch: 020 | Train Loss: 0.42881 | Val Loss: 0.52145 | Accuracy: 0.77457 | Dice: 0.76311\n"]},{"output_type":"execute_result","data":{"text/plain":["([0.6623961806297303,\n","  0.6029936790466308,\n","  0.5861643195152283,\n","  0.5686008095741272,\n","  0.5579383969306946,\n","  0.5412316679954529,\n","  0.5409008026123047,\n","  0.5269254803657532,\n","  0.5261570990085602,\n","  0.5268876135349274,\n","  0.5012646794319153,\n","  0.49479838609695437,\n","  0.5088325679302216,\n","  0.4711315333843231,\n","  0.4607066512107849,\n","  0.4586978256702423,\n","  0.45997313857078553,\n","  0.4681236743927002,\n","  0.4371532559394836,\n","  0.4288052201271057],\n"," [0.6870650053024292,\n","  0.6860554218292236,\n","  0.6833656430244446,\n","  0.6651206314563751,\n","  0.6521838009357452,\n","  0.6405759751796722,\n","  0.6030705571174622,\n","  0.5995907485485077,\n","  0.6114628911018372,\n","  0.5895787477493286,\n","  0.5911286175251007,\n","  0.7068969011306763,\n","  0.6075293719768524,\n","  0.5750120133161545,\n","  0.6526180803775787,\n","  0.6998774111270905,\n","  0.5596853494644165,\n","  0.6752162277698517,\n","  0.5065233260393143,\n","  0.5214511752128601],\n"," [tensor(0.4916, device='cuda:0'),\n","  tensor(0.5485, device='cuda:0'),\n","  tensor(0.5964, device='cuda:0'),\n","  tensor(0.6781, device='cuda:0'),\n","  tensor(0.6950, device='cuda:0'),\n","  tensor(0.5179, device='cuda:0'),\n","  tensor(0.6624, device='cuda:0'),\n","  tensor(0.6325, device='cuda:0'),\n","  tensor(0.6307, device='cuda:0'),\n","  tensor(0.6527, device='cuda:0'),\n","  tensor(0.6976, device='cuda:0'),\n","  tensor(0.6481, device='cuda:0'),\n","  tensor(0.7083, device='cuda:0'),\n","  tensor(0.6949, device='cuda:0'),\n","  tensor(0.6989, device='cuda:0'),\n","  tensor(0.7049, device='cuda:0'),\n","  tensor(0.7524, device='cuda:0'),\n","  tensor(0.6348, device='cuda:0'),\n","  tensor(0.7931, device='cuda:0'),\n","  tensor(0.7631, device='cuda:0')])"]},"metadata":{},"execution_count":19}],"source":["model = UNET(3,64, 2).to(DEVICE)\n","\n","train_unet(\n","    model=model,\n","    optimizer=optim.SGD(model.parameters(), lr=0.01),\n","    criterion=nn.CrossEntropyLoss().to(DEVICE),\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    device=DEVICE,\n","    do_early_stopping=True,\n","    patience=5,\n","    epochs=20,\n","    log_fn=print_log_unet,\n","    log_every=1,\n",")"]},{"cell_type":"code","source":["def rle_encode(mask):\n","  pixels = np.array(mask).flatten(order='F')  # Aplanar la máscara en orden Fortran\n","  pixels = np.concatenate([[0], pixels, [0]])  # Añadir ceros al principio y final\n","  runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n","  runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n","  return ' '.join(str(x) for x in runs)\n","\n","def submit_kaggle_results():\n","  !kaggle competitions submit -c tdl-segmentacion -f submission.csv -m \"By Germán Otero\"\n","\n","def plot_image_and_mask(image, mask):\n","  plt.figure(figsize=(10,5))\n","  img=image.permute(1,2,0).to(DEVICE).cpu().numpy()\n","  mask=mask.permute(1,2,0).to(DEVICE).cpu().numpy()\n","  plt.imshow(img)\n","  plt.axis('off')\n","  plt.imshow(mask, alpha=0.7, cmap='gray',vmin=0,vmax=1)\n","  plt.axis('off')\n","  plt.tight_layout()\n","  plt.show()\n","\n","def generate_csv_for_submission(images,masks):\n","  import csv\n","\n","  # Construimos la lista que será utilizada para la escritura\n","  # del CSV.\n","  # Se coloca como primera línea el encabezado de acuerdo\n","  # a las instrucciones de Kaggle para la competencia.\n","  data = [[\"id\", \"encoded_pixels\"]]\n","  for i in range(len(images)):\n","    data.append([images[i], rle_encode(masks[i])])\n","\n","  # Write to CSV\n","  with open('submission_.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(data)\n","\n","  print(f\"La cantidad de datos son: {len(data)}\")\n","  print(\"CSV generado correctamente.\")\n","\n","\n","def generate_kaggle_submission(trained_model, device, test_path):\n","  \"\"\"\n","    trained_model: recibe el modelo entrenado que será utilizado para evaluación\n","    device: modelo dónde se realizarán los cálculos de tensores\n","    test_path: donde se encuentran las imágenes de test\n","  \"\"\"\n","\n","  # Definimos un par de listas para guardar el nombre\n","  # de la imagen, y su respectiva máscara\n","  images_names = []\n","  masks = []\n","  # Preparamos el modelo para evaluación\n","  model = trained_model\n","  model.eval()\n","  model.to(device)\n","\n","  images = sorted(os.listdir(test_path))\n","  # transform = T.ToTensor()\n","\n","  # Recorremos las imágenes, y evaluamos cada una de éstas\n","  with torch.no_grad():\n","    for img in images:\n","      images_names.append(img)\n","      # Cargamos la imagen\n","      img_path = os.path.join(test_path, img)\n","      image = read_image(img_path)\n","      # Agregamos una dimensión asociada al batch\n","      image = image.unsqueeze(dim=0)\n","      # Tenemos la imagen como tensor, y de un tamaño 800x800\n","      # Procedemos a pasarla por el modelo\n","      x = image.to(device=device, dtype = torch.float32)\n","      scores = model(x)\n","      # Obtenemos las predicciones\n","      preds = torch.argmax(scores, dim=1)\n","      # Agregamos la máscara a la lista de máscaras\n","      masks.append(preds.cpu().numpy())\n","      # plot_image_and_mask(image.squeeze(0), preds)\n","\n","  generate_csv_for_submission(images_names, masks)"],"metadata":{"id":"bKI4vSYcsoVC","executionInfo":{"status":"ok","timestamp":1733763624009,"user_tz":180,"elapsed":280,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Subimos los datos a Kaggle\n","generate_kaggle_submission(model, DEVICE, TEST_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nP98oHrkyi9c","executionInfo":{"status":"ok","timestamp":1733763668151,"user_tz":180,"elapsed":33370,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"7a54cd50-97d3-400e-953e-9fdbc18a6b53"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["La cantidad de datos son: 535\n","CSV generado correctamente.\n"]}]},{"cell_type":"code","source":["submit_kaggle_results()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_uXzEoQXk-q","executionInfo":{"status":"ok","timestamp":1733763715409,"user_tz":180,"elapsed":2754,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"d2bdb1c7-9076-41c7-dee0-c634fdff0a06"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 38.9M/38.9M [00:01<00:00, 40.3MB/s]\n","Successfully submitted to [TDL] Segmentación"]}]},{"cell_type":"markdown","source":["# Análisis de resultados\n","\n","Como se puede observar, el dice obtenido no supera el valor de 0.66 por lo que comenzaremos a mejorar los datos mediante data augmentation.\n","\n"],"metadata":{"id":"36eQ0VfACoWV"}},{"cell_type":"code","source":["# Vamos a comenzar aplicando un \"flip\" vertical a las imágenes, sobre un\n","# 100% del total de las imágenes y máscaras, y duplicar el tamaño\n","# del dataset original.\n","\n","image_transforms = T.Compose([\n","    T.Resize((224, 224)),\n","    T.RandomVerticalFlip(p=1),\n","])\n","\n","mask_transforms = T.Compose([\n","    T.Resize((224, 224)),\n","    T.RandomVerticalFlip(p=1),\n","])\n","\n","full_dataset_flipped = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=image_transforms, mask_transforms=mask_transforms)\n","\n","TRAIN_SIZE = int(len(full_dataset_flipped)*0.8)\n","VAL_SIZE = len(full_dataset_flipped) - TRAIN_SIZE\n","\n","train_dataset_flipped, val_dataset_flipped = random_split(full_dataset_flipped, [TRAIN_SIZE, VAL_SIZE])\n","\n","# Concatenamos con los datasets originales, para obtener un nuevo dataset\n","# con nuevas imágenes modificadas (al igual que sus máscaras)\n","\n","train_dataset_agumented = ConcatDataset([train_dataset, train_dataset_flipped])\n","val_dataset_agumented = ConcatDataset([val_dataset, val_dataset_flipped])\n","\n","# Obtenemos los nuevos dataloaders\n","\n","train_loader_augmented = DataLoader(train_dataset_agumented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","val_loader_augmented = DataLoader(val_dataset_agumented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"],"metadata":{"id":"2er2039QDX2k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora, contamos con un nuevo dataset, que contiene el doble de elementos que originalmente, y a su vez, la mitad de las imágenes fueron \"flippeadas\"\n","horizontalmente.\n","\n","Mostremos algunas de estas imágenes"],"metadata":{"id":"4AZgupKxHBDG"}},{"cell_type":"code","source":["imgs_a, masks_a = next(iter(train_loader_augmented))\n","plot_mini_batch(imgs_a, masks_a, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1CFwpEIZAFmLUfM0wPmm5DbhbNhyWnHlO"},"id":"rJPQRz1NH-fq","executionInfo":{"status":"ok","timestamp":1733719694761,"user_tz":180,"elapsed":4862,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"42cd1cbe-b87f-4d89-8563-46fc9ba4cc90"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Procederemos a volver a entrenar el modelo"],"metadata":{"id":"-dhqpjSvKvBS"}},{"cell_type":"code","source":["model_vertically_flipped = UNET(3,64, 2).to(DEVICE)\n","\n","train_unet(\n","    model=model_vertically_flipped,\n","    optimizer=optim.SGD(model_vertically_flipped.parameters(), lr=0.01),\n","    criterion=nn.CrossEntropyLoss().to(DEVICE),\n","    train_loader=train_loader_augmented,\n","    val_loader=val_loader_augmented,\n","    device=DEVICE,\n","    epochs=20,\n","    log_fn=print_log_unet,\n","    log_every=1,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWle8LmdK0vc","executionInfo":{"status":"ok","timestamp":1733719998759,"user_tz":180,"elapsed":297067,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"288738f2-2e08-4a1e-e5fc-6e6be2a605c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001 | Train Loss: 0.70433 | Val Loss: 0.69379 | Accuracy: 0.45202 | Dice: 0.57120\n","Epoch: 002 | Train Loss: 0.69899 | Val Loss: 0.69441 | Accuracy: 0.50522 | Dice: 0.55844\n","Epoch: 003 | Train Loss: 0.70383 | Val Loss: 0.69892 | Accuracy: 0.49368 | Dice: 0.44457\n","Epoch: 004 | Train Loss: 0.70139 | Val Loss: 0.69772 | Accuracy: 0.59791 | Dice: 0.31968\n","Epoch: 005 | Train Loss: 0.70681 | Val Loss: 0.73317 | Accuracy: 0.36923 | Dice: 0.26127\n","Epoch: 006 | Train Loss: 0.70487 | Val Loss: 0.72056 | Accuracy: 0.53836 | Dice: 0.20596\n","Epoch: 007 | Train Loss: 0.70405 | Val Loss: 0.71586 | Accuracy: 0.50121 | Dice: 0.31435\n","Epoch: 008 | Train Loss: 0.70226 | Val Loss: 0.72523 | Accuracy: 0.47632 | Dice: 0.32968\n","Epoch: 009 | Train Loss: 0.70390 | Val Loss: 0.71835 | Accuracy: 0.49206 | Dice: 0.25764\n","Epoch: 010 | Train Loss: 0.70396 | Val Loss: 0.72248 | Accuracy: 0.50633 | Dice: 0.25919\n","Epoch: 011 | Train Loss: 0.70311 | Val Loss: 0.71181 | Accuracy: 0.55628 | Dice: 0.35939\n","Epoch: 012 | Train Loss: 0.70847 | Val Loss: 0.71690 | Accuracy: 0.54904 | Dice: 0.31418\n","Epoch: 013 | Train Loss: 0.70081 | Val Loss: 0.70849 | Accuracy: 0.58960 | Dice: 0.36684\n","Epoch: 014 | Train Loss: 0.70095 | Val Loss: 0.71640 | Accuracy: 0.54343 | Dice: 0.30455\n","Epoch: 015 | Train Loss: 0.70259 | Val Loss: 0.71649 | Accuracy: 0.52919 | Dice: 0.32952\n","Epoch: 016 | Train Loss: 0.70489 | Val Loss: 0.73375 | Accuracy: 0.40455 | Dice: 0.18943\n","Epoch: 017 | Train Loss: 0.70631 | Val Loss: 0.72005 | Accuracy: 0.51326 | Dice: 0.25760\n","Epoch: 018 | Train Loss: 0.70652 | Val Loss: 0.71604 | Accuracy: 0.48525 | Dice: 0.26229\n","Epoch: 019 | Train Loss: 0.70612 | Val Loss: 0.72970 | Accuracy: 0.43994 | Dice: 0.25352\n","Epoch: 020 | Train Loss: 0.70424 | Val Loss: 0.72937 | Accuracy: 0.43076 | Dice: 0.29449\n"]},{"output_type":"execute_result","data":{"text/plain":["([0.7043270885944366,\n","  0.6989946603775025,\n","  0.7038322985172272,\n","  0.7013931393623352,\n","  0.7068085312843323,\n","  0.7048747062683105,\n","  0.7040525436401367,\n","  0.7022562265396118,\n","  0.7038958311080933,\n","  0.7039627730846405,\n","  0.7031134128570556,\n","  0.7084710240364075,\n","  0.7008111238479614,\n","  0.7009522080421448,\n","  0.7025912582874299,\n","  0.7048943161964416,\n","  0.7063143074512481,\n","  0.706518805027008,\n","  0.7061226904392243,\n","  0.704237163066864],\n"," [],\n"," [])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Como se puede observar, no existe una notoria mejoría, por lo que vamos a proceder a crear una serie de transformaciones, a efectos de aumentar el dataset con imágenes modificadas. Crearemos funciones que irán incrementando el dataset, el que será utilizado posteriormente para entrenar el modelo.\n","\n","De acuerdo al paper de Olaf Ronneberger \"et al\", en la página 3 se hace mención al uso de \"data augmentation\" como estrategia frente a la ausencia de datos. Más específicamente, citamos a continuación lo mencionado en el paper: \"... we use excessive data augmentation by applying elastic deformations to the available training images.\". La explicación de esta técnica, tiene su base en cómo las imágenes son obtenidas.\n","\n","Bajo esta teoría, y considerando las variaciones de las \"personas\" en las fotos, podríamos considerar que las principales variaciones radican en:\n","- Variación en el color de las personas, encontrándose diferente color de piel, de vestimenta, etc.\n","- Variación del tamaño de la persona dentro de la imagen.\n","- Variación de la posición de la persona en la imagen, respecto a los ejes x e y, presentándose diferentes grados de inclinación de la persona respecto a la horizontalidad y verticalidad de las imágenes.\n","\n","Por lo anterior, se aplicarán las siguientes transformaciones sobre las imágenes:\n","- Aplicación \"fotométrica\" de \"ColorJittering\" para modificar el contraste, saturación, y otras propiedades colorométricas de la imagen.\n","- Aplicación \"geométrica\" de una transformación \"Crop\" aumentándose de forma aleatoria ciertas secciones de la imagen, con el objetivo de simular la presencia de tamaños mayores y menores de las personas dentro de las imágenes.\n","- Aplicación \"geométrica\" de la rotación de la imagen, con el objetivo de simular la posición de las personas dentro de las imágenes.\n","\n","Se crearán por lo anterior 3 transformers, y se aplicarán estos de forma aleatoria sobre el dataset.\n","\n","Se utiliza como referencia la documentación propia de Pytorch para la manipulación de imágenes: https://pytorch.org/vision/0.19/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n"],"metadata":{"id":"QarFmr-zNgw0"}},{"cell_type":"code","source":["# Se define una tupla con las dimensiones a utilizar durante el entrenamiento.\n","# Se pudo comprobar -mediante experiencia- que para valores superiores a\n","# 224 x 224, la memoria en Colab no es suficiente para el procesamiento.\n","IMAGE_DIMENSIONS = (224,224)\n","\n","# Transformación 1:\n","# No es necesaria aplicarla sobre la máscara al no modificarse\n","# la geometría de la imagen.\n","image_transforms_color_jitter = T.Compose([\n","    T.Resize(IMAGE_DIMENSIONS),\n","    T.ColorJitter(brightness=.3, hue=.3)\n","])\n","\n","# Transformación 2:\n","# Es necesario aplicar la misma transformación a la máscara\n","# por tratarse de una transformación geométrica\n","image_transforms_center_crop = T.Compose([\n","    T.Resize(IMAGE_DIMENSIONS),\n","    T.CenterCrop(size=90),\n","    T.Resize(IMAGE_DIMENSIONS),\n","])\n","\n","# Transformación 3:\n","# Aplicamos una rotación de 30 grados, que también\n","# debe aplicarse a la máscara\n","image_transforms_rotate = T.Compose([\n","    T.RandomRotation(degrees=(15,15)),\n","    T.Resize(IMAGE_DIMENSIONS),\n","])\n","\n","# Creamos los datasets que luego serán concatenados\n","full_dataset_transformed1 = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=image_transforms_color_jitter, mask_transforms=T.Resize(IMAGE_DIMENSIONS))\n","full_dataset_transformed2 = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=image_transforms_center_crop, mask_transforms=image_transforms_center_crop)\n","full_dataset_transformed3 = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=image_transforms_rotate, mask_transforms=image_transforms_rotate)\n","full_dataset_without_transforms = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=T.Resize(IMAGE_DIMENSIONS), mask_transforms=T.Resize(IMAGE_DIMENSIONS))\n","\n","# Concatenamos los datasets considerando un 50% de las imágenes transformadas\n","total_size = len(full_dataset_without_transforms)\n","subset_size = round(total_size * 0.5)\n","indices = np.random.choice(total_size, subset_size, replace=False)\n","\n","full_dataset_transformed1 = Subset(full_dataset_transformed1, indices)\n","full_dataset_transformed2 = Subset(full_dataset_transformed2, indices)\n","full_dataset_transformed3 = Subset(full_dataset_transformed3, indices)\n","\n","full_dataset_augmented = ConcatDataset([full_dataset_transformed1, full_dataset_transformed2, full_dataset_transformed3, full_dataset_without_transforms])\n","\n","TRAIN_SIZE = int(len(full_dataset_augmented)*0.8)\n","VAL_SIZE = len(full_dataset_augmented) - TRAIN_SIZE\n","\n","train_dataset_augmented, val_dataset_augmented = random_split(full_dataset_augmented, [TRAIN_SIZE, VAL_SIZE])\n","\n","# Obtenemos los nuevos dataloaders\n","\n","train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)\n","val_loader_augmented = DataLoader(val_dataset_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, drop_last=True)"],"metadata":{"id":"nXV0q7--Nyf_","executionInfo":{"status":"ok","timestamp":1733764290551,"user_tz":180,"elapsed":370,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Mostramos un minibatch de imágenes transformadas\n","imgs_a, masks_a = next(iter(train_loader_augmented))\n","plot_mini_batch(imgs_a, masks_a, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17b3bdKaeQ8sl_EGry2VLAcBc6gfQ6agU"},"id":"Xq0EKvnIXGXe","executionInfo":{"status":"ok","timestamp":1733764298202,"user_tz":180,"elapsed":5000,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"3dbc4604-bd1c-4da8-bf4b-66962ffbbcbf"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Volvemos a entrenar el modelo:"],"metadata":{"id":"i_d8p5fnYB7q"}},{"cell_type":"code","source":["model_augmented = UNET(3,64, 2).to(DEVICE)\n","\n","train_unet(\n","    model=model_augmented,\n","    optimizer=optim.SGD(model_augmented.parameters(), lr=0.01),\n","    criterion=nn.CrossEntropyLoss().to(DEVICE),\n","    train_loader=train_loader_augmented,\n","    val_loader=val_loader_augmented,\n","    device=DEVICE,\n","    do_early_stopping=True,\n","    patience=5,\n","    epochs=20,\n","    log_fn=print_log_unet,\n","    log_every=2,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FKjx45rYEE-","executionInfo":{"status":"ok","timestamp":1733769422334,"user_tz":180,"elapsed":5119553,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"73c88db5-b3f5-4263-d095-9718096a6f8c"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 002 | Train Loss: 0.49255 | Val Loss: 0.45616 | Accuracy: 0.78479 | Dice: 0.75057\n","Epoch: 004 | Train Loss: 0.41709 | Val Loss: 0.44950 | Accuracy: 0.77417 | Dice: 0.77953\n","Epoch: 006 | Train Loss: 0.37296 | Val Loss: 0.55561 | Accuracy: 0.73579 | Dice: 0.67047\n","Epoch: 008 | Train Loss: 0.34223 | Val Loss: 0.33542 | Accuracy: 0.83902 | Dice: 0.79784\n","Epoch: 010 | Train Loss: 0.30621 | Val Loss: 0.49435 | Accuracy: 0.78042 | Dice: 0.80687\n","Epoch: 012 | Train Loss: 0.28430 | Val Loss: 0.43791 | Accuracy: 0.77727 | Dice: 0.70149\n","Epoch: 014 | Train Loss: 0.25995 | Val Loss: 0.35464 | Accuracy: 0.85848 | Dice: 0.84578\n","Epoch: 016 | Train Loss: 0.23101 | Val Loss: 0.53959 | Accuracy: 0.75693 | Dice: 0.67113\n","Epoch: 018 | Train Loss: 0.20784 | Val Loss: 0.27583 | Accuracy: 0.85864 | Dice: 0.84255\n","Epoch: 020 | Train Loss: 0.18843 | Val Loss: 0.36861 | Accuracy: 0.87006 | Dice: 0.84079\n"]},{"output_type":"execute_result","data":{"text/plain":["([0.5870013662746975,\n","  0.4925451699952434,\n","  0.44492256910281075,\n","  0.4170925749423809,\n","  0.3901124036401734,\n","  0.3729621261582339,\n","  0.35542898137766615,\n","  0.3422344362825379,\n","  0.32551546092320205,\n","  0.30620921296732767,\n","  0.29681818590576486,\n","  0.2843029339958851,\n","  0.2686294060676618,\n","  0.25995152090725143,\n","  0.2429346928471013,\n","  0.23100713097063222,\n","  0.22415926649158163,\n","  0.20783617767624388,\n","  0.19926512448635317,\n","  0.1884344596611826],\n"," [0.5438188025445649,\n","  0.4561562312371803,\n","  0.4475801704507886,\n","  0.4494963089625041,\n","  0.3950656932411772,\n","  0.5556059136535182,\n","  0.36719912109952985,\n","  0.3354159440055038,\n","  0.3616174269806255,\n","  0.494350018826398,\n","  0.39974710525888385,\n","  0.43790959499099036,\n","  0.3478669424851735,\n","  0.354637266108484,\n","  0.321079078948859,\n","  0.5395897961024082,\n","  0.3565561419183558,\n","  0.27583248326272675,\n","  0.6444794138272604,\n","  0.36860591173171997],\n"," [tensor(0.6498, device='cuda:0'),\n","  tensor(0.7506, device='cuda:0'),\n","  tensor(0.7677, device='cuda:0'),\n","  tensor(0.7795, device='cuda:0'),\n","  tensor(0.8500, device='cuda:0'),\n","  tensor(0.6705, device='cuda:0'),\n","  tensor(0.8055, device='cuda:0'),\n","  tensor(0.7978, device='cuda:0'),\n","  tensor(0.8184, device='cuda:0'),\n","  tensor(0.8069, device='cuda:0'),\n","  tensor(0.8166, device='cuda:0'),\n","  tensor(0.7015, device='cuda:0'),\n","  tensor(0.8487, device='cuda:0'),\n","  tensor(0.8458, device='cuda:0'),\n","  tensor(0.8095, device='cuda:0'),\n","  tensor(0.6711, device='cuda:0'),\n","  tensor(0.8109, device='cuda:0'),\n","  tensor(0.8425, device='cuda:0'),\n","  tensor(0.6502, device='cuda:0'),\n","  tensor(0.8408, device='cuda:0')])"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["# Competencia Kaggle\n","\n","Luego de diversas pruebas, y habiendo obtenido un modelo con valores aceptables de \"dice\" en las pruebas locales, se procede a subir los datos a la competencia de Kaggle.\n","\n","Se crean algunas funciones auxiliares, así como también la necesaria para el \"encoding\" de los datos mediante \"RLE\"."],"metadata":{"id":"iHDTDZ6usGLP"}},{"cell_type":"code","source":["def rle_encode(mask):\n","  '''\n","  Función que codifica la máscara pasada como parámetro en RLE\n","  '''\n","  pixels = np.array(mask).flatten(order='F')  # Aplanar la máscara en orden Fortran\n","  pixels = np.concatenate([[0], pixels, [0]])  # Añadir ceros al principio y final\n","  runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n","  runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n","  return ' '.join(str(x) for x in runs)\n","\n","def submit_kaggle_results():\n","  '''\n","  Función que permite subir los resultados a Kaggle\n","  '''\n","  !kaggle competitions submit -c tdl-segmentacion -f submission.csv -m \"By Germán Otero\"\n","\n","def plot_image_and_mask(image, mask):\n","  '''\n","  Función que muestra una imágen, y su máscara permitiendo visualmente\n","  comparar la predicción del modelo.\n","  '''\n","  plt.figure(figsize=(10,5))\n","  img=image.permute(1,2,0).to(DEVICE).cpu().numpy()\n","  mask=mask.permute(1,2,0).to(DEVICE).cpu().numpy()\n","  plt.imshow(img)\n","  plt.axis('off')\n","  plt.imshow(mask, alpha=0.7, cmap='gray',vmin=0,vmax=1)\n","  plt.axis('off')\n","  plt.tight_layout()\n","  plt.show()\n","\n","def generate_csv_for_submission(images,masks):\n","  '''\n","  Método que genera un CSV que será utilizado posteriormente\n","  para la competencia de Kaggle.\n","  '''\n","  import csv\n","\n","  # Construimos la lista que será utilizada para la escritura\n","  # del CSV.\n","  # Se coloca como primera línea el encabezado de acuerdo\n","  # a las instrucciones de Kaggle para la competencia.\n","  data = [[\"id\", \"encoded_pixels\"]]\n","  for i in range(len(images)):\n","    data.append([images[i], rle_encode(masks[i])])\n","\n","  # Write to CSV\n","  with open('submission_.csv', 'w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerows(data)\n","\n","  print(f\"La cantidad de datos son: {len(data)}\")\n","  print(\"CSV generado correctamente.\")\n","\n","\n","def generate_kaggle_submission(trained_model, device, test_path):\n","  \"\"\"\n","    trained_model: recibe el modelo entrenado que será utilizado para evaluación\n","    device: modelo dónde se realizarán los cálculos de tensores\n","    test_path: donde se encuentran las imágenes de test\n","  \"\"\"\n","\n","  # Definimos un par de listas para guardar el nombre\n","  # de la imagen, y su respectiva máscara\n","  images_names = []\n","  masks = []\n","  # Preparamos el modelo para evaluación\n","  model = trained_model\n","  model.eval()\n","  model.to(device)\n","\n","  images = sorted(os.listdir(test_path))\n","  # transform = T.ToTensor()\n","\n","  # Recorremos las imágenes, y evaluamos cada una de éstas\n","  with torch.no_grad():\n","    counter = 0\n","    for img in images:\n","      images_names.append(img)\n","      # Cargamos la imagen\n","      img_path = os.path.join(test_path, img)\n","      image = read_image(img_path)\n","      # Agregamos una dimensión asociada al batch\n","      image = image.unsqueeze(dim=0)\n","      # Tenemos la imagen como tensor, y de un tamaño 800x800\n","      # Procedemos a pasarla por el modelo\n","      x = image.to(device=device, dtype = torch.float32)\n","      scores = model(x)\n","      # Obtenemos las predicciones\n","      preds = torch.argmax(scores, dim=1)\n","      # Agregamos la máscara a la lista de máscaras\n","      masks.append(preds.cpu().numpy())\n","      if counter % 25 == 0:\n","        plot_image_and_mask(image.squeeze(0), preds)\n","      counter += 1\n","\n","  generate_csv_for_submission(images_names, masks)"],"metadata":{"id":"WZWlKkhfsciO","executionInfo":{"status":"ok","timestamp":1733769693023,"user_tz":180,"elapsed":221,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# Generamos CSV\n","generate_kaggle_submission(model, DEVICE, TEST_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"9HAEHdnhti3d","executionInfo":{"status":"error","timestamp":1733769832782,"user_tz":180,"elapsed":215,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"d0a5d0b2-840f-4216-e451-e1a4177c17b9"},"execution_count":53,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-f12cfbfd5516>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generamos CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_kaggle_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEST_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":["# Subimos los datos a Kaggle\n","submit_kaggle_results()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LBbVteNtlLy","executionInfo":{"status":"ok","timestamp":1733769752697,"user_tz":180,"elapsed":2357,"user":{"displayName":"Germán Otero","userId":"03275765356545836121"}},"outputId":"cea4034b-126e-4761-f726-480c0e297081"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 17.5M/17.5M [00:00<00:00, 32.8MB/s]\n","Successfully submitted to [TDL] Segmentación"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}