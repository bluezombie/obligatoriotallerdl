{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZzGt0PbUIGG"
      },
      "source": [
        "# Obligatorio - Taller de Deep Learning\n",
        "\n",
        "**Fecha de entrega:** 10/12/2024  \n",
        "**Puntaje máximo:** 50 puntos  \n",
        "\n",
        "## Obligatorio\n",
        "\n",
        "El objetivo de este obligatorio es evaluar su conocimiento en Deep Learning mediante la implementación completa de un modelo de segmentación de imágenes basado en el paper [**\"U-Net: Convolutional Networks for Biomedical Image Segmentation\"**](https://arxiv.org/pdf/1505.04597). Toda la implementación debe realizarse desde cero utilizando PyTorch, y los estudiantes tendrán la libertad de ajustar ciertos hiperparámetros y configuraciones mientras mantengan la esencia del paper original.\n",
        "\n",
        "### **Competencia en Kaggle**\n",
        "\n",
        "Además, como parte de este obligatorio, participarán en una competencia privada en Kaggle donde se les proporcionará un dataset de test oculto (sin target). Deberán subir sus predicciones a Kaggle y se evaluarán en función de la métrica **Dice Coefficient (Coeficiente de Dice)**. Esta competencia les permitirá comparar sus resultados con los de sus compañeros en un entorno real de evaluación.\n",
        "\n",
        "### **Qué es el Dice Coefficient?**\n",
        "El **Dice Coefficient**, también conocido como F1-score para segmentación, es una métrica utilizada para evaluar la similitud entre la predicción y la verdad del terreno en tareas de segmentación. Se define de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\text{Dice} = \\frac{2 \\cdot |A \\cap B|}{|A| + |B|}\n",
        "$$\n",
        "\n",
        "Donde:\n",
        "- \\(A\\) es el conjunto de píxeles predichos como pertenecientes a la clase positiva.\n",
        "- \\(B\\) es el conjunto de píxeles verdaderos pertenecientes a la clase positiva.\n",
        "- \\(|A \\cap B|\\) es la intersección de \\(A\\) y \\(B\\), es decir, los píxeles correctamente predichos como positivos.\n",
        "\n",
        "Un valor de Dice de **1** indica una predicción perfecta, mientras que un valor de **0** indica que no hay coincidencia entre la predicción y el valor verdadero. Durante la competencia de Kaggle, deberán obtener un puntaje de al menos **0.7** en la métrica Dice para considerarse aprobados.\n",
        "\n",
        "### **Criterios a Evaluar**\n",
        "\n",
        "1. **Implementación Correcta del Modelo U-Net (20 puntos):**\n",
        "   - Construcción de la arquitectura U-Net siguiendo la estructura descrita en el paper, permitiendo ajustes como el número de filtros, funciones de activación y métodos de inicialización de pesos.\n",
        "   - Se aceptan mejoras como el uso de técnicas adicionales como batch normalization, otras funciones de activación, etc.\n",
        "\n",
        "2. **Entrenamiento del Modelo (10 puntos):**\n",
        "   - Configuración adecuada del ciclo de entrenamiento, incluyendo la elección de la función de pérdida y del optimizador (Adam, SGD, etc.).\n",
        "   - Uso de técnicas de regularización para mejorar la generalización del modelo, como el dropout, normalización de batch y data augmentation.\n",
        "   - Gráficas y análisis de la evolución del entrenamiento, mostrando las curvas de pérdida y métricas relevantes tanto en el conjunto de entrenamiento como en el de validación.\n",
        "\n",
        "3. **Evaluación de Resultados (10 puntos):**\n",
        "   - Evaluación exhaustiva del modelo utilizando métricas de segmentación como **Dice Coefficient**.\n",
        "   - Análisis detallado de los resultados, incluyendo un análisis de errores para identificar y discutir casos difíciles.\n",
        "   - Visualización de ejemplos representativos de segmentaciones correctas e incorrectas, comparando con las etiquetas manuales proporcionadas en el dataset.\n",
        "\n",
        "4. **Participación y Resultados en la Competencia Kaggle (5 puntos):**\n",
        "   - Participación activa en la competencia de Kaggle, con al menos una (1) subida de predicción.\n",
        "   - Puntaje obtenido en la tabla de posiciones de Kaggle, evaluado en base al **Dice Coefficient** en el conjunto de test oculto. Es necesario obtener al menos un valor de **0.7** para esta métrica.\n",
        "\n",
        "   Nota: El **Dice Coefficient** es la métrica utilizada para evaluar la precisión de los modelos de segmentación de imágenes en esta competencia. Un valor de Dice superior a 0.7 es requerido para aprobar esta tarea.\n",
        "\n",
        "### **Run-Length Encoding (RLE)**\n",
        "\n",
        "Dado que no se suben las imágenes segmentadas directamente a Kaggle, se requiere usar **Run-Length Encoding (RLE)** para comprimir las máscaras de predicción en una cadena de texto que será evaluada. El **RLE** es una técnica de compresión donde se representan secuencias consecutivas de píxeles en formato `start length`, indicando la posición de inicio y la longitud de cada secuencia de píxeles positivos.\n",
        "\n",
        "Para calcular el **RLE**, se sigue el siguiente proceso:\n",
        "\n",
        "1. Se aplanan las máscaras predichas en un solo vector\n",
        "2. Se identifican los píxeles con valor positivo (1) y se calculan las secuencias consecutivas.\n",
        "3. Se registra la posición de inicio de cada secuencia y su longitud en formato `start length`.\n",
        "\n",
        "Este formato comprimido se sube a Kaggle en lugar de las imágenes segmentadas.\n",
        "\n",
        "#### **Ejemplo de RLE**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(mask):\n",
        "    pixels = np.array(mask).flatten(order='F')  # Aplanar la máscara en orden Fortran\n",
        "    pixels = np.concatenate([[0], pixels, [0]])  # Añadir ceros al principio y final\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1  # Encontrar transiciones\n",
        "    runs[1::2] = runs[1::2] - runs[::2]  # Calcular longitudes\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "mask = np.array([[0, 0, 1, 0, 0],\n",
        "                 [0, 1, 1, 1, 0],\n",
        "                 [1, 1, 1, 0, 0],\n",
        "                 [0, 0, 0, 1, 1]])\n",
        "\n",
        "print(rle_encode(mask))\n",
        "```\n",
        "\n",
        "> **Salida:** 3 1 6 2 9 3 14 1 16 1 20 1\n",
        "\n",
        "\n",
        "### **Sobre el Dataset**\n",
        "\n",
        "El dataset proporcionado para esta tarea incluirá imágenes y máscaras para la segmentación de un conjunto específico de clases. El conjunto de entrenamiento estará disponible para su uso durante todo el proceso de desarrollo y pruebas, mientras que el conjunto de validación se mantendrá oculto para la evaluación final en Kaggle.\n",
        "\n",
        "### **Instrucciones de Entrega**\n",
        "\n",
        "- Deberán entregar un Jupyter Notebook (.ipynb) que contenga todo el código y las explicaciones necesarias para ejecutar la implementación, el entrenamiento y la evaluación del modelo.\n",
        "- El notebook debe incluir secciones bien documentadas explicando las decisiones de diseño del modelo, los experimentos realizados, y los resultados obtenidos.\n",
        "- El código debe estar escrito de manera clara.\n",
        "- La entrega debe realizarse a través de la plataforma de gestión de ORT (gestion.ort.edu.uy) antes de la fecha límite.\n",
        "\n",
        "### **Materiales Adicionales**\n",
        "\n",
        "Para facilitar su trabajo, pueden consultar los siguientes recursos:\n",
        "\n",
        "- [U-Net: Convolutional Networks for Biomedical Image Segmentation (paper original)](https://arxiv.org/abs/1505.04597)\n",
        "- [Documentación de PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "- [Tutoriales y recursos adicionales en Kaggle](https://www.kaggle.com/)\n",
        "\n",
        "### **Competencia Kaggle**\n",
        "\n",
        "https://www.kaggle.com/t/9b4e546084034a59b182aac1ae892640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bDg55HyUIGR"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX1bBPyfUIGR"
      },
      "source": [
        "\n",
        "## Requisitos Universitarios\n",
        "\n",
        "Fecha de entrega: 10/12/2024 hasta las 21:00 horas en gestion.ort.edu.uy (max. 40Mb en formato zip)\n",
        "\n",
        "### Uso de material de apoyo y/o consulta\n",
        "\n",
        "Inteligencia Artificial Generativa:\n",
        "\n",
        "   - Seguir las pautas de los docentes: Se deben seguir las instrucciones específicas de los docentes sobre cómo utilizar la IA en cada curso.\n",
        "   - Citar correctamente las fuentes y usos de IA: Siempre que se utilice una herramienta de IA para generar contenido, se debe citar adecuadamente la fuente y la forma en que se utilizó.\n",
        "   - Verificar el contenido generado por la IA: No todo el contenido generado por la IA es correcto o preciso. Es esencial que los estudiantes verifiquen la información antes de usarla.\n",
        "   - Ser responsables con el uso de la IA: Conocer los riesgos y desafíos, como la creación de “alucinaciones”, los peligros para la privacidad, las cuestiones de propiedad intelectual, los sesgos inherentes y la producción de contenido falso.\n",
        "   - En caso de existir dudas sobre la autoría, plagio o uso no atribuido de IAG, el docente tendrá la opción de convocar al equipo de obligatorio a una defensa específica e individual sobre el tema.\n",
        "\n",
        "### Defensa\n",
        "\n",
        "Fecha de defensa: 11/12/2024\n",
        "\n",
        "La defensa es obligatoria y eliminatoria. El docente es quien definirá y comunicará la modalidad, y mecánica de defensa. La no presentación a la misma implica la pérdida de la totalidad de los puntos del Obligatorio.\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "   1) Inscribirse\n",
        "   2) Formar grupos de hasta 2 personas del mismo dictado\n",
        "   3) Subir el trabajo a Gestión antes de la hora indicada (ver hoja al final del documento: “RECORDATORIO”)\n",
        "\n",
        "Aquellos de ustedes que presenten alguna dificultad con su inscripción o tengan inconvenientes técnicos, por favor contactarse con el Coordinador de cursos o Coordinación adjunta antes de las 20:00h del día de la entrega, a través de los mails crosa@ort.edu.uy / posada_l@ort.edu.uy (matutino) / larrosa@ort.edu.uy (nocturno), o vía Ms Teams."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación entorno"
      ],
      "metadata": {
        "id": "8bH1rWpba_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXb6EO3JVPsM",
        "outputId": "1b0e5d69-7985-4eae-f857-5241a5070a01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo\n",
        "!pip install scikit-learn\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtegMesabVSN",
        "outputId": "08d79584-d5a1-4ba6-a306-3bb054305f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el entorno para interactar con Kaggle\n",
        "# Debemos subir el archivo kaggle.json previamente\n",
        "!mkdir -p /root/.config/kaggle\n",
        "!mv kaggle.json /root/.config/kaggle/\n",
        "!chmod 600 /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "X80xVHuDmHhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from torchvision.transforms import v2 as T\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "import kaggle\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "from utils import (\n",
        "    train,\n",
        "    model_calassification_report,\n",
        "    show_tensor_image,\n",
        "    show_tensor_images,\n",
        ")"
      ],
      "metadata": {
        "id": "ljwM0SF0bBat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos el dispositivo que vamos a usar\n",
        "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
        "\n",
        "#NUM_WORKERS = max(os.cpu_count() - 1, 1)\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "# DEVICE = \"cpu\"\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(f\"Num Workers: {NUM_WORKERS}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZslx9wPbhBO",
        "outputId": "cd84e6c8-b1cf-4a29-fcc7-4856a4c99322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Num Workers: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# De acuerdo a la documentación de Colab, obtenemos información de la GPU\n",
        "# en caso de contar con ella.\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfQXb6nkXvfj",
        "outputId": "e6a8fdb2-1750-4b65-e5bf-416be6294313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  7 21:25:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes\n",
        "\n",
        "SEED = 34\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "h-_obzs1dt2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset & Dataloader"
      ],
      "metadata": {
        "id": "9uIx_1A4Y3cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PeopleDataset(Dataset):\n",
        "    def __init__(self, data, masks=None, img_transforms=None, mask_transforms=None):\n",
        "      self.train_data = data\n",
        "      self.train_masks = masks\n",
        "      self.img_transforms = img_transforms\n",
        "      self.mask_transforms = mask_transforms\n",
        "\n",
        "      self.images = sorted(os.listdir(self.train_data))\n",
        "      self.masks = sorted(os.listdir(self.train_masks))\n",
        "\n",
        "      print(f\"Valor de la transformación pasada como parámetro: \",mask_transforms)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      if self.train_masks is not None:\n",
        "        assert len(self.images) == len(self.masks), 'incompatibilidad de imágenes y máscaras'\n",
        "      return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # Obtenemos los paths de las imágenes\n",
        "      img_path = os.path.join(self.train_data, self.images[idx])\n",
        "      mask_path = os.path.join(self.train_masks, self.masks[idx])\n",
        "\n",
        "      # Abrimos las imágenes y máscara\n",
        "      # image = Image.open(img_path)\n",
        "      # mask = Image.open(mask_path)\n",
        "\n",
        "      image = read_image(img_path)\n",
        "      mask = read_image(mask_path)\n",
        "\n",
        "      # Verificamos si existen transformaciones, y en caso que sí\n",
        "      # las aplicamos\n",
        "      if self.img_transforms:\n",
        "        image = self.img_transforms(image)\n",
        "      if self.mask_transforms is not None:\n",
        "        mask = self.mask_transforms(mask)\n",
        "\n",
        "\n",
        "      img = image.to(DEVICE)\n",
        "      # Nos quedamos con una sola capa de la imagen, ya que todas son iguales\n",
        "      msk = mask[0].to(DEVICE)\n",
        "\n",
        "      return img, msk"
      ],
      "metadata": {
        "id": "AnmE9vXdY5O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el dataset\n",
        "!kaggle competitions download -c tdl-segmentacion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaHnkDKwjjES",
        "outputId": "01618f42-ac88-406b-b42b-6bc817a11682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tdl-segmentacion.zip to /content\n",
            "100% 2.14G/2.14G [01:39<00:00, 23.5MB/s]\n",
            "100% 2.14G/2.14G [01:39<00:00, 23.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomprimimos el dataset\n",
        "!unzip -q tdl-segmentacion.zip"
      ],
      "metadata": {
        "id": "4UHevwfqn4KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = 'train/images/'\n",
        "TRAIN_MASK_PATH = 'train/masks/'\n",
        "TEST_PATH = 'test/images/'\n",
        "\n",
        "transform_data = T.Compose([\n",
        "   T.Resize((224, 224))])\n",
        "\n",
        "target_transform = T.Compose([\n",
        "    T.Resize((224, 224))])\n",
        "\n",
        "full_dataset = PeopleDataset(data=TRAIN_PATH, masks=TRAIN_MASK_PATH, img_transforms=transform_data, mask_transforms=target_transform)\n",
        "\n",
        "TRAIN_SIZE = int(len(full_dataset)*0.8)\n",
        "VAL_SIZE = len(full_dataset) - TRAIN_SIZE\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [TRAIN_SIZE, VAL_SIZE])\n",
        "\n",
        "test_dataset = PeopleDataset(data=TEST_PATH, masks=None, img_transforms=transform_data, mask_transforms=None)\n",
        "\n",
        "\n",
        "def get_data_loaders(batch_size):\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxwANDv0ZPYm",
        "outputId": "691c2d21-701d-45cf-9fae-63b84148f4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor de la transformación pasada como parámetro:  Compose(    Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True))\n",
            "Valor de la transformación pasada como parámetro:  None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos los dataloaders\n",
        "train_loader, val_loader, test_loader = get_data_loaders(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "c9ZHzYRCp3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testeamos el dataloader\n",
        "# obteniendo un minibatch\n",
        "imgs, masks = next(iter(train_loader))\n",
        "for i in range(len(imgs)):\n",
        "  print(imgs[i].shape, masks[i].shape)\n",
        "print(imgs.shape, masks.shape)\n",
        "print(imgs.min(), imgs.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNOFFuxWXi_V",
        "outputId": "dffa8d7d-2d37-4da6-c042-85968018bb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([3, 224, 224]) torch.Size([224, 224])\n",
            "torch.Size([32, 3, 224, 224]) torch.Size([32, 224, 224])\n",
            "tensor(0, device='cuda:0', dtype=torch.uint8) tensor(255, device='cuda:0', dtype=torch.uint8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_mini_batch(imgs, masks, show_mask=True):\n",
        "  '''\n",
        "  Función para mostrar un mini batch de imágenes y máscaras\n",
        "  '''\n",
        "  plt.figure(figsize=(20,10))\n",
        "  for i in range(BATCH_SIZE):\n",
        "    plt.subplot(4, 8, i+1)\n",
        "    # Permutamos al tratarse de un tensor\n",
        "    img=imgs[i,...].permute(1,2,0).to(DEVICE).cpu().numpy()\n",
        "    mask=masks[i,...].to(DEVICE).cpu().numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    # Incluimos la máscara\n",
        "    if show_mask:\n",
        "      plt.imshow(mask[0], alpha=0.6, cmap='gray',vmin=0,vmax=1)\n",
        "    plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ZkwiJr87n2-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostramos un mini batch de las imágenes\n",
        "# plot_mini_batch(imgs, masks, True)\n",
        "\n",
        "# show_tensor_image(img[BATCH_SIZE-1], title=\"Imágen\", vmin=0, vmax=255)\n",
        "\n",
        "# show_tensor_images(imgs, title=\"Imágen\", vmin=0, vmax=255)\n",
        "show_tensor_images(imgs, titles=\"Ejemplos\", figsize=(15, 5), vmin=0, vmax=255)\n",
        "# Mostramos a continuación que cualquiera de las capas\n",
        "# de la máscara, contienen el mismo valor\n",
        "# plt.imshow(masks[BATCH_SIZE-1][2], cmap='gray', vmin=0, vmax=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "c0_7QKi3Vc6T",
        "outputId": "2e38385d-b858-4f49-973d-55775ab6a4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d0755d438185>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# show_tensor_images(imgs, title=\"Imágen\", vmin=0, vmax=255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshow_tensor_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Ejemplos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# Mostramos a continuación que cualquiera de las capas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# de la máscara, contienen el mismo valor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mshow_tensor_images\u001b[0;34m(tensors, titles, figsize, vmin, vmax)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Assume RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtitles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5749\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5751\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5752\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_image_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mImage\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \"\"\"\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"same_kind\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             raise TypeError(f\"Image data of dtype {A.dtype} cannot be \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# If we have already made a copy, do the byteswap in place, else make a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 32 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo\n"
      ],
      "metadata": {
        "id": "vDlCkTOEUe14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dada la estructura de la U-Net, y la repetición de los bloques de\n",
        "convolución tanto en la sección de down-sampling como en la de\n",
        "up-sampling, crearemos clases con la estructura de la\n",
        "convolución. De esta forma, simplificaremos el\n",
        "código parametrizando dichas convoluciones\n",
        "de acuerdo a la etapa de la red en la\n",
        "que nos encontremos."
      ],
      "metadata": {
        "id": "DcL2_-3nw6HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Conv_3x3_block(nn.Module):\n",
        "#   def __init__(self, channels_in, channels_out):\n",
        "#     super(Conv_3x3_block, self).__init__()\n",
        "#     # Utilizaremos padding para que las imágenes no se modifiquen\n",
        "#     self.conv1 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1)\n",
        "#   def forward(self, x):\n",
        "#     return self.conv1(x)"
      ],
      "metadata": {
        "id": "sETT_-neUnG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Double_Conv(nn.Module):\n",
        "#   def __init__(self, channels_in, channels_out):\n",
        "#     super(Double_Conv, self).__init__()\n",
        "#     self.double_conv = nn.Sequential(\n",
        "#         Conv_3x3_block(channels_in, channels_out),\n",
        "#         # Vamos a normalizar los datos para facilitar\n",
        "#         # el entrenamiento.\n",
        "#         nn.BatchNorm2d(channels_out),\n",
        "#         nn.ReLU(),\n",
        "#         Conv_3x3_block(channels_out, channels_out),\n",
        "#         nn.BatchNorm2d(channels_out),\n",
        "#         nn.ReLU()\n",
        "#     )\n",
        "#   def forward(self, x):\n",
        "#     return self.double_conv(x)"
      ],
      "metadata": {
        "id": "IAqJAOxY0FSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Vamos a comenzar con la primera sección de la U-Net\n",
        "# class Down_conv_encoder(nn.Module):\n",
        "#   def __init__(self,  channels_in, channels_out):\n",
        "#     super(Down_conv_encoder, self).__init__()\n",
        "#     self.down_sampling = nn.Sequential(\n",
        "#                         nn.MaxPool2d(2,2),\n",
        "#                         Double_Conv(channels_in, channels_out)\n",
        "#                       )\n",
        "#   def forward(self, x):\n",
        "#     return self.down_sampling(x)"
      ],
      "metadata": {
        "id": "h9Y4EO6q2gEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ahora continuamos con la sección del up-sampling de la U-Net\n",
        "# class Up_conv_decoder(nn.Module):\n",
        "#   def __init__(self, channels_in, channels_out):\n",
        "#     super(Up_conv_decoder, self).__init__()\n",
        "#     self.up_sampling = nn.Sequential(\n",
        "#                         #nn.Upsample(scale_factor=2, mode='bicubic'),\n",
        "#                         nn.ConvTranspose2d(channels_in, channels_out, kernel_size=2, stride=2),\n",
        "#                         nn.Conv2d(channels_in, channels_in//2, kernel_size=1, stride=1)\n",
        "#                         )\n",
        "#     self.decoder=Double_Conv(channels_in, channels_out)\n",
        "\n",
        "#     # x2: representa el skip connection\n",
        "#     #     proveniente de la sección la\n",
        "#     #     etapa de down sampling\n",
        "\n",
        "#   def forward(self, x1, x2):\n",
        "#     x1 = self.up_sampling(x1)\n",
        "#     # Concatenamos en la dimensión de los canales\n",
        "#     # La dim=0 corresponde con la dimensión\n",
        "#     # del batch.\n",
        "#     x = torch.cat([x2, x1], dim=1)\n",
        "#     return self.decoder(x)\n"
      ],
      "metadata": {
        "id": "81ecA_Np3hJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, vamos a ensamblar los diferentes componentes para finalmente\n",
        "armar la U-Net de acuerdo al paper original."
      ],
      "metadata": {
        "id": "SL75-T87V7xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class UNET(nn.Module):\n",
        "#   def __init__(self, channels_in, channels, num_classes):\n",
        "#     super(UNET, self).__init__()\n",
        "#     self.encoder_1 = Double_Conv(channels_in, channels)\n",
        "#     self.encoder_2 = Down_conv_encoder(channels, 2*channels)\n",
        "#     self.encoder_3 = Down_conv_encoder(2*channels, 4*channels)\n",
        "#     self.encoder_4 = Down_conv_encoder(4*channels, 8*channels)\n",
        "#     self.encoder_5 = Down_conv_encoder(8*channels, 16*channels)\n",
        "\n",
        "#     self.decoder_1 = Up_conv_decoder(16*channels, 8*channels)\n",
        "#     self.decoder_2 = Up_conv_decoder(8*channels, 4*channels)\n",
        "#     self.decoder_3 = Up_conv_decoder(4*channels, 2*channels)\n",
        "#     self.decoder_4 = Up_conv_decoder(2*channels, channels)\n",
        "\n",
        "#     self.last_conv = nn.Conv2d(channels, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x1 = self.encoder_1(x)\n",
        "#     x2 = self.encoder_2(x1)\n",
        "#     x3 = self.encoder_3(x2)\n",
        "#     x4 = self.encoder_4(x3)\n",
        "\n",
        "#     x5 = self.encoder_5(x4)\n",
        "\n",
        "#     u1 = self.decoder_1(x5, x4)\n",
        "#     u2 = self.decoder_2(u1, x3)\n",
        "#     u3 = self.decoder_3(u2, x2)\n",
        "#     u4 = self.decoder_4(u3, x1)\n",
        "\n",
        "#     return self.last_conv(u4)"
      ],
      "metadata": {
        "id": "znhHacA0Am_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_3_k(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=1, padding=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv1(x)\n",
        "\n",
        "class Double_Conv(nn.Module):\n",
        "    '''\n",
        "    Double convolution block for U-Net\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "                           Conv_3_k(channels_in, channels_out),\n",
        "                           nn.BatchNorm2d(channels_out),\n",
        "                           nn.ReLU(),\n",
        "\n",
        "                           Conv_3_k(channels_out, channels_out),\n",
        "                           nn.BatchNorm2d(channels_out),\n",
        "                           nn.ReLU(),\n",
        "                            )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down_Conv(nn.Module):\n",
        "    '''\n",
        "    Down convolution part\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "                        nn.MaxPool2d(2,2),\n",
        "                        Double_Conv(channels_in, channels_out)\n",
        "                        )\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Up_Conv(nn.Module):\n",
        "    '''\n",
        "    Up convolution part\n",
        "    '''\n",
        "    def __init__(self,channels_in, channels_out):\n",
        "        super().__init__()\n",
        "        self.upsample_layer = nn.Sequential(\n",
        "                        nn.Upsample(scale_factor=2, mode='bicubic'),\n",
        "                        nn.Conv2d(channels_in, channels_in//2, kernel_size=1, stride=1)\n",
        "                        )\n",
        "        self.decoder = Double_Conv(channels_in, channels_out)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        '''\n",
        "        x1 - upsampled volume\n",
        "        x2 - volume from down sample to concatenate\n",
        "        '''\n",
        "        x1 = self.upsample_layer(x1)\n",
        "        x = torch.cat([x2, x1],dim=1)\n",
        "        return self.decoder(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    '''\n",
        "    UNET model\n",
        "    '''\n",
        "    def __init__(self, channels_in, channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.first_conv = Double_Conv(channels_in, channels) #64, 224, 224\n",
        "        self.down_conv1 = Down_Conv(channels, 2*channels) # 128, 112, 112\n",
        "        self.down_conv2 = Down_Conv(2*channels, 4*channels) # 256, 56, 56\n",
        "        self.down_conv3 = Down_Conv(4*channels, 8*channels) # 512, 28, 28\n",
        "\n",
        "        self.middle_conv = Down_Conv(8*channels, 16*channels) # 1024, 14, 14\n",
        "\n",
        "        self.up_conv1 = Up_Conv(16*channels, 8*channels)\n",
        "        self.up_conv2 = Up_Conv(8*channels, 4*channels)\n",
        "        self.up_conv3 = Up_Conv(4*channels, 2*channels)\n",
        "        self.up_conv4 = Up_Conv(2*channels, channels)\n",
        "\n",
        "        self.last_conv = nn.Conv2d(channels, num_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.first_conv(x)\n",
        "        x2 = self.down_conv1(x1)\n",
        "        x3 = self.down_conv2(x2)\n",
        "        x4 = self.down_conv3(x3)\n",
        "\n",
        "        x5 = self.middle_conv(x4)\n",
        "\n",
        "        u1 = self.up_conv1(x5, x4)\n",
        "        u2 = self.up_conv2(u1, x3)\n",
        "        u3 = self.up_conv3(u2, x2)\n",
        "        u4 = self.up_conv4(u3, x1)\n",
        "\n",
        "        return self.last_conv(u4)"
      ],
      "metadata": {
        "id": "-gSjF2zCWv1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(UNET(3, 64, 2), input_size=(BATCH_SIZE, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPmCKtrbZXml",
        "outputId": "99d1b311-7ac5-45c4-c958-e0806de99f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "UNET                                          [32, 2, 224, 224]         --\n",
              "├─Double_Conv: 1-1                            [32, 64, 224, 224]        --\n",
              "│    └─Sequential: 2-1                        [32, 64, 224, 224]        --\n",
              "│    │    └─Conv_3_k: 3-1                     [32, 64, 224, 224]        1,792\n",
              "│    │    └─BatchNorm2d: 3-2                  [32, 64, 224, 224]        128\n",
              "│    │    └─ReLU: 3-3                         [32, 64, 224, 224]        --\n",
              "│    │    └─Conv_3_k: 3-4                     [32, 64, 224, 224]        36,928\n",
              "│    │    └─BatchNorm2d: 3-5                  [32, 64, 224, 224]        128\n",
              "│    │    └─ReLU: 3-6                         [32, 64, 224, 224]        --\n",
              "├─Down_Conv: 1-2                              [32, 128, 112, 112]       --\n",
              "│    └─Sequential: 2-2                        [32, 128, 112, 112]       --\n",
              "│    │    └─MaxPool2d: 3-7                    [32, 64, 112, 112]        --\n",
              "│    │    └─Double_Conv: 3-8                  [32, 128, 112, 112]       221,952\n",
              "├─Down_Conv: 1-3                              [32, 256, 56, 56]         --\n",
              "│    └─Sequential: 2-3                        [32, 256, 56, 56]         --\n",
              "│    │    └─MaxPool2d: 3-9                    [32, 128, 56, 56]         --\n",
              "│    │    └─Double_Conv: 3-10                 [32, 256, 56, 56]         886,272\n",
              "├─Down_Conv: 1-4                              [32, 512, 28, 28]         --\n",
              "│    └─Sequential: 2-4                        [32, 512, 28, 28]         --\n",
              "│    │    └─MaxPool2d: 3-11                   [32, 256, 28, 28]         --\n",
              "│    │    └─Double_Conv: 3-12                 [32, 512, 28, 28]         3,542,016\n",
              "├─Down_Conv: 1-5                              [32, 1024, 14, 14]        --\n",
              "│    └─Sequential: 2-5                        [32, 1024, 14, 14]        --\n",
              "│    │    └─MaxPool2d: 3-13                   [32, 512, 14, 14]         --\n",
              "│    │    └─Double_Conv: 3-14                 [32, 1024, 14, 14]        14,161,920\n",
              "├─Up_Conv: 1-6                                [32, 512, 28, 28]         --\n",
              "│    └─Sequential: 2-6                        [32, 512, 28, 28]         --\n",
              "│    │    └─Upsample: 3-15                    [32, 1024, 28, 28]        --\n",
              "│    │    └─Conv2d: 3-16                      [32, 512, 28, 28]         524,800\n",
              "│    └─Double_Conv: 2-7                       [32, 512, 28, 28]         --\n",
              "│    │    └─Sequential: 3-17                  [32, 512, 28, 28]         7,080,960\n",
              "├─Up_Conv: 1-7                                [32, 256, 56, 56]         --\n",
              "│    └─Sequential: 2-8                        [32, 256, 56, 56]         --\n",
              "│    │    └─Upsample: 3-18                    [32, 512, 56, 56]         --\n",
              "│    │    └─Conv2d: 3-19                      [32, 256, 56, 56]         131,328\n",
              "│    └─Double_Conv: 2-9                       [32, 256, 56, 56]         --\n",
              "│    │    └─Sequential: 3-20                  [32, 256, 56, 56]         1,771,008\n",
              "├─Up_Conv: 1-8                                [32, 128, 112, 112]       --\n",
              "│    └─Sequential: 2-10                       [32, 128, 112, 112]       --\n",
              "│    │    └─Upsample: 3-21                    [32, 256, 112, 112]       --\n",
              "│    │    └─Conv2d: 3-22                      [32, 128, 112, 112]       32,896\n",
              "│    └─Double_Conv: 2-11                      [32, 128, 112, 112]       --\n",
              "│    │    └─Sequential: 3-23                  [32, 128, 112, 112]       443,136\n",
              "├─Up_Conv: 1-9                                [32, 64, 224, 224]        --\n",
              "│    └─Sequential: 2-12                       [32, 64, 224, 224]        --\n",
              "│    │    └─Upsample: 3-24                    [32, 128, 224, 224]       --\n",
              "│    │    └─Conv2d: 3-25                      [32, 64, 224, 224]        8,256\n",
              "│    └─Double_Conv: 2-13                      [32, 64, 224, 224]        --\n",
              "│    │    └─Sequential: 3-26                  [32, 64, 224, 224]        110,976\n",
              "├─Conv2d: 1-10                                [32, 2, 224, 224]         130\n",
              "===============================================================================================\n",
              "Total params: 28,954,626\n",
              "Trainable params: 28,954,626\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (T): 1.18\n",
              "===============================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 14103.87\n",
              "Params size (MB): 115.82\n",
              "Estimated Total Size (MB): 14238.96\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como puede observarse en el resumen, el modelo y su arquitectura hacen computacionalmente pesado el proceso. De lo anterior, puede observarse\n",
        "que se requieren unos 14 GB de memoria RAM aproximadamente por cada\n",
        "\"forward\" de un batch. Esto podría tener implicancias en el entrenamiento\n",
        "ya que dados los recursos de hardware, es posible que se tengan que\n",
        "variar tanto el BATCH_SIZE, como el tamaño de las imágenes a procesar."
      ],
      "metadata": {
        "id": "Pso29u7yZnZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos una función de test, y observamos el shape\n",
        "# del modelo luego de la predicción\n",
        "def test():\n",
        "    x = torch.randn((32, 3, 224, 224))\n",
        "    model = UNET(3, 64, 2)\n",
        "    return model(x)\n",
        "\n",
        "preds = test()\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czP9VsleMPsA",
        "outputId": "c94084ea-0ea6-4f52-e09e-3c366364917b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento\n",
        "\n",
        "Probemos inicialmente un entrenamiento del modelo, sin realizar ninguna manipulación de las imágenes más allá de modificar su tamaño a 224x224 para mantener un tamaño reducido y no sobrecargar la memoria."
      ],
      "metadata": {
        "id": "0TqOT9izaWmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    intersection = 0\n",
        "    denom = 0\n",
        "    union = 0\n",
        "    total = 0\n",
        "    cost = 0.\n",
        "    model = model.to(device=device)\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=DEVICE, dtype = torch.float32)\n",
        "            y = y.to(device=DEVICE, dtype = torch.long).squeeze(1)\n",
        "            scores = model(x)\n",
        "            cost += (F.cross_entropy(scores, y)).item()\n",
        "            # standard accuracy not optimal\n",
        "            preds = torch.argmax(scores, dim=1)\n",
        "            correct += (preds == y).sum()\n",
        "            total += torch.numel(preds)\n",
        "            #dice coefficient\n",
        "            intersection += (preds*y).sum()\n",
        "            denom += (preds + y).sum()\n",
        "            dice = 2*intersection/(denom + 1e-8)\n",
        "            #intersection over union\n",
        "            union += (preds + y - preds*y).sum()\n",
        "            iou = (intersection)/(union + 1e-8)\n",
        "\n",
        "        return cost/len(loader), float(correct)/total, dice, iou"
      ],
      "metadata": {
        "id": "ndYMYCjSh8O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, optimiser, scheduler = None, epochs = 100, store_every = 25):\n",
        "    model = model.to(device=DEVICE)\n",
        "    for epoch in range(epochs):\n",
        "        train_correct_num = 0\n",
        "        train_total = 0\n",
        "        train_cost_acum = 0.\n",
        "        for mb, (x, y) in enumerate(train_loader, start=1):\n",
        "            model.train()\n",
        "            x = x.to(device=DEVICE, dtype=torch.float32)\n",
        "            y = y.to(device=DEVICE, dtype=torch.long).squeeze(1)\n",
        "            scores = model(x)\n",
        "            cost = F.cross_entropy(input=scores, target=y)\n",
        "            optimiser.zero_grad()\n",
        "            cost.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "\n",
        "            train_predictions = torch.argmax(scores, dim=1)\n",
        "            train_correct_num += (train_predictions == y).sum()\n",
        "            train_total += torch.numel(train_predictions)\n",
        "            train_cost_acum += cost.item()\n",
        "            if mb%store_every == 0:\n",
        "                val_cost, val_acc, dice, iou = accuracy(model, val_loader)\n",
        "                train_acc = float(train_correct_num)/train_total\n",
        "                train_cost_every = float(train_cost_acum)/mb\n",
        "                print(f'epoch: {epoch}, mb: {mb}, train cost: {train_cost_every:.4f}, val cost: {val_cost:.4f},'\n",
        "                      f'train acc: {train_acc:.4f}, val acc: {val_acc:.4f},'\n",
        "                      f'dice: {dice:.4f}, iou: {iou:.4f}')\n",
        "                    # Save data\n",
        "                    #train_acc_history.append(train_acc)\n",
        "                    #train_cost_history.append(train_cost_every)\n",
        "        #train_acc = float(train_correct_num)/train_total\n",
        "        #train_cost_every = float(train_cost_acum)/len(train_loader)\n",
        "        # return train_acc_history ... etc"
      ],
      "metadata": {
        "id": "zxOBa-wOhv3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_lr(model, optimiser, start_val = 1e-6, end_val = 1, beta = 0.99, loader = train_loader):\n",
        "    n = len(loader) - 1\n",
        "    factor = (end_val / start_val)**(1/n)\n",
        "    lr = start_val\n",
        "    optimiser.param_groups[0]['lr'] = lr #this allows you to update the learning rate\n",
        "    avg_loss, loss, acc = 0., 0., 0.\n",
        "    lowest_loss = 0.\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    accuracies = []\n",
        "    model = model.to(device=DEVICE)\n",
        "    for i, (x, y) in enumerate(loader, start=1):\n",
        "        x = x.to(device = DEVICE, dtype = torch.float32)\n",
        "        y = y.to(device = DEVICE, dtype = torch.long).squeeze(1)\n",
        "        optimiser.zero_grad()\n",
        "        scores = model(x)\n",
        "        cost = F.cross_entropy(input=scores, target=y)\n",
        "        loss = beta*loss + (1-beta)*cost.item()\n",
        "        #bias correction\n",
        "        avg_loss = loss/(1 - beta**i)\n",
        "\n",
        "        preds = torch.argmax(scores, dim=1)\n",
        "        acc_ = (preds == y).sum()/torch.numel(scores)\n",
        "#         acc = beta*acc + (1-beta)*acc_.item()\n",
        "#         avg_acc = acc/(1 - beta**i)\n",
        "        #if loss is massive stop\n",
        "        if i > 1 and avg_loss > 4 * lowest_loss:\n",
        "            print(f'from here{i, cost.item()}')\n",
        "            return log_lrs, losses, accuracies\n",
        "        if avg_loss < lowest_loss or i == 1:\n",
        "            lowest_loss = avg_loss\n",
        "\n",
        "        accuracies.append(acc_.item())\n",
        "#         accuracies.append(avg_acc)\n",
        "        losses.append(avg_loss)\n",
        "        log_lrs.append(lr)\n",
        "        #step\n",
        "        cost.backward()\n",
        "        optimiser.step()\n",
        "        #update lr\n",
        "        print(f'cost:{cost.item():.4f}, lr: {lr:.4f}, acc: {acc_.item():.4f}')\n",
        "        lr *= factor\n",
        "        optimiser.param_groups[0]['lr'] = lr\n",
        "\n",
        "    return log_lrs, losses, accuracies"
      ],
      "metadata": {
        "id": "Qb7uZKuni1A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model and look for learning rate\n",
        "torch.manual_seed(42)\n",
        "model = UNET(3, 4, 2)\n",
        "optimiser_unet = torch.optim.SGD(model.parameters(),\n",
        "                                 lr=0.01, momentum=0.95,\n",
        "                                 weight_decay=1e-4)\n",
        "\n",
        "lg_lr, losses, accuracies = find_lr(model, optimiser_unet, start_val=1e-6, end_val=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Sbw6pQiwSv",
        "outputId": "008effec-ed9e-4190-de3b-69376fe15a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost:0.7389, lr: 0.0000, acc: 0.2478\n",
            "cost:0.7493, lr: 0.0001, acc: 0.2330\n",
            "cost:0.7018, lr: 0.0032, acc: 0.2847\n",
            "cost:0.6986, lr: 0.1778, acc: 0.2762\n",
            "cost:0.7225, lr: 10.0000, acc: 0.2588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = UNET(3,64, 2).to(DEVICE)\n",
        "\n",
        "train_unet(\n",
        "    model=model,\n",
        "    optimizer=optim.SGD(model.parameters(), lr=0.01),\n",
        "    criterion=nn.CrossEntropyLoss().to(DEVICE),\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=DEVICE,\n",
        "    do_early_stopping=True,\n",
        "    patience=5,\n",
        "    epochs=10,\n",
        "    log_fn=utils.print_log,\n",
        "    log_every=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "XZ2cZFVMajtW",
        "outputId": "21b74dca-f53d-4cfb-8793-7f4ee1f96967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'utils' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-75cd7f45fbfe>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlog_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mlog_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}